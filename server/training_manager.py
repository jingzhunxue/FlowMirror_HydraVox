import os
import subprocess
import time
import json
import threading
from typing import Dict, Any, Optional, List
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

class TrainingManager:
    """è®­ç»ƒç®¡ç†å™¨ï¼Œè´Ÿè´£å¯åŠ¨ã€ç›‘æ§å’Œç®¡ç†è®­ç»ƒè¿›ç¨‹"""
    
    def __init__(self, log_dir: str = "logs/training"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        self.active_trainings: Dict[str, Dict[str, Any]] = {}
        self.lock = threading.Lock()
    
    def start_training(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """å¯åŠ¨è®­ç»ƒä»»åŠ¡
        
        Args:
            config: è®­ç»ƒé…ç½®å‚æ•°
            
        Returns:
            è®­ç»ƒä»»åŠ¡ä¿¡æ¯
        """
        training_id = f"train_{int(time.time())}"
        log_file = self.log_dir / f"{training_id}.log"
        
        try:
            # æ„å»ºè®­ç»ƒå‘½ä»¤
            cmd = self._build_training_command(config)
            
            # å¯åŠ¨è®­ç»ƒè¿›ç¨‹ï¼Œè®¾ç½®æ–°çš„è¿›ç¨‹ç»„
            process = subprocess.Popen(
                cmd,
                shell=True,
                stdout=open(log_file, 'w', encoding='utf-8'),
                stderr=subprocess.STDOUT,
                cwd=Path(__file__).parent.parent,  # é¡¹ç›®æ ¹ç›®å½•
                preexec_fn=os.setsid  # åˆ›å»ºæ–°çš„è¿›ç¨‹ç»„
            )
            
            with self.lock:
                self.active_trainings[training_id] = {
                    'process': process,
                    'log_file': str(log_file),
                    'status': 'running',
                    'start_time': time.time(),
                    'config': config,
                    'pid': process.pid,
                    'last_log_position': 0
                }
            
            logger.info(f"Started training {training_id} with PID {process.pid}")
            
            return {
                'training_id': training_id,
                'status': 'running',
                'pid': process.pid,
                'log_file': str(log_file),
                'message': 'è®­ç»ƒå·²å¯åŠ¨'
            }
            
        except Exception as e:
            logger.error(f"Failed to start training: {e}")
            return {
                'training_id': None,
                'status': 'failed',
                'error': str(e),
                'message': f'è®­ç»ƒå¯åŠ¨å¤±è´¥: {e}'
            }
    
    def _build_training_command(self, config: Dict[str, Any]) -> str:
        """æ„å»ºè®­ç»ƒå‘½ä»¤"""
        logger.info("=" * 60)
        logger.info("ğŸ”¨ å¼€å§‹æ„å»ºè®­ç»ƒå‘½ä»¤")
        logger.info("æ¥æ”¶åˆ°çš„configå‚æ•°:")
        for key, value in config.items():
            logger.info(f"  {key}: {value}")
        logger.info("-" * 60)
        
        # ä½¿ç”¨accelerateå¯åŠ¨è®­ç»ƒï¼ŒæŒ‡å®šé…ç½®æ–‡ä»¶
        accelerate_config_path = "configs/accelerate_default_config.yaml"
        base_cmd = f"accelerate launch --config_file {accelerate_config_path} scripts/train/train_speech_model.py"
        
        logger.info(f"ğŸš€ ä½¿ç”¨accelerateå¯åŠ¨è®­ç»ƒ")
        logger.info(f"ğŸ“‹ accelerateé…ç½®æ–‡ä»¶: {accelerate_config_path}")
        
        # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        config_full_path = os.path.join(os.getcwd(), accelerate_config_path)
        if not os.path.exists(config_full_path):
            logger.warning(f"âš ï¸ accelerateé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_full_path}")
        else:
            logger.info(f"âœ… accelerateé…ç½®æ–‡ä»¶å­˜åœ¨: {config_full_path}")
        
        # å¿…éœ€å‚æ•° - ç§»é™¤configå‚æ•°ï¼Œè®©è„šæœ¬è‡ªåŠ¨ä½¿ç”¨é»˜è®¤è·¯å¾„
        required_params = [
            f"--model {config.get('model_type', 'llm')}",
            f"--model_ckpt {config.get('model_checkpoint', 'jzx-ai-lab/HydraVox/llm.pt')}",
            f"--tokenizer_path {config.get('tokenizer_path', 'jzx-ai-lab/HydraVox/speech_tokenizer')}",
            f"--train_data {config.get('train_data', '')}",
            f"--output_dir {config.get('output_dir', 'checkpoints/training')}"
        ]
        
        # å¯é€‰å‚æ•°
        optional_params = []
        
        if config.get('cv_data'):
            optional_params.append(f"--cv_data {config['cv_data']}")
        
        if config.get('auto_val_split', False):
            optional_params.append("--auto_val_split")
            optional_params.append(f"--val_split_ratio {config.get('val_split_ratio', 0.05)}")
        
        # è®­ç»ƒå‚æ•° - éªŒè¯batch_sizeè‡ªåŠ¨ä¸è®­ç»ƒbatch_sizeä¸€è‡´
        batch_size = config.get('batch_size', 4)
        optional_params.extend([
            f"--per_device_train_batch_size {batch_size}",
            f"--per_device_eval_batch_size {batch_size}",  # éªŒè¯batch_sizeä¸è®­ç»ƒä¸€è‡´
            f"--learning_rate {config.get('learning_rate', 1e-4)}",
            f"--num_train_epochs {config.get('epochs', 10)}",
            f"--gradient_accumulation_steps {config.get('gradient_accumulation_steps', 1)}",
            f"--logging_steps {config.get('logging_steps', 50)}",
            f"--eval_steps {config.get('eval_steps', 1000)}",
            f"--save_steps {config.get('save_steps', 2000)}",
            f"--dataloader_num_workers {config.get('dataloader_num_workers', 8)}"
        ])
        
        # ç²¾åº¦è®¾ç½® - æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©åˆé€‚çš„ç²¾åº¦
        model_type = config.get('model_type', 'llm')
        if config.get('use_fp16', False):
            optional_params.append("--fp16")
        elif config.get('use_bf16', False):
            optional_params.append("--bf16")
        else:
            # å¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šç²¾åº¦ï¼Œæ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©é»˜è®¤ç²¾åº¦
            if model_type == 'llm':
                optional_params.append("--bf16")  # LLMæ¨èBF16
            elif model_type == 'flow':
                optional_params.append("--fp16")  # Flowæ¨èFP16
        
        # LoRAè®¾ç½®
        if config.get('enable_lora', False):
            optional_params.extend([
                "--enable_lora",
                f"--lora_r {config.get('lora_r', 64)}",
                f"--lora_alpha {config.get('lora_alpha', 128)}",
                f"--lora_dropout {config.get('lora_dropout', 0.05)}"
            ])
        
        # DeepSpeedè®¾ç½®
        if config.get('deepspeed_config'):
            optional_params.append(f"--deepspeed {config['deepspeed_config']}")
        
        all_params = required_params + optional_params
        final_command = f"{base_cmd} {' '.join(all_params)}"
        
        logger.info("æ„å»ºçš„è®­ç»ƒå‘½ä»¤:")
        logger.info(f"  {final_command}")
        logger.info("=" * 60)
        
        return final_command
    
    def stop_training(self, training_id: str) -> Dict[str, Any]:
        """åœæ­¢è®­ç»ƒä»»åŠ¡"""
        with self.lock:
            if training_id not in self.active_trainings:
                return {
                    'success': False,
                    'message': f'è®­ç»ƒä»»åŠ¡ {training_id} ä¸å­˜åœ¨'
                }
            
            training = self.active_trainings[training_id]
            process = training['process']
            pid = training['pid']
            
            try:
                if process.poll() is None:  # è¿›ç¨‹ä»åœ¨è¿è¡Œ
                    # é¦–å…ˆå°è¯•ä¼˜é›…åœ°ç»ˆæ­¢æ•´ä¸ªè¿›ç¨‹ç»„
                    try:
                        import signal
                        os.killpg(os.getpgid(pid), signal.SIGTERM)
                        logger.info(f"Sent SIGTERM to process group of PID {pid}")
                    except Exception as e:
                        logger.warning(f"Failed to kill process group: {e}, trying individual process")
                        process.terminate()
                    
                    # ç­‰å¾…è¿›ç¨‹ç»“æŸï¼Œå¦‚æœ5ç§’åä»æœªç»“æŸåˆ™å¼ºåˆ¶æ€æ­»
                    try:
                        process.wait(timeout=5)
                        logger.info(f"Training process {pid} terminated gracefully")
                    except subprocess.TimeoutExpired:
                        logger.warning(f"Training process {pid} did not terminate, force killing")
                        try:
                            # å¼ºåˆ¶æ€æ­»è¿›ç¨‹ç»„
                            os.killpg(os.getpgid(pid), signal.SIGKILL)
                            logger.info(f"Sent SIGKILL to process group of PID {pid}")
                        except Exception as e:
                            logger.warning(f"Failed to force kill process group: {e}")
                            process.kill()
                        process.wait()
                
                training['status'] = 'stopped'
                training['end_time'] = time.time()
                
                logger.info(f"Stopped training {training_id}")
                
                return {
                    'success': True,
                    'message': f'è®­ç»ƒä»»åŠ¡ {training_id} å·²åœæ­¢'
                }
                
            except Exception as e:
                logger.error(f"Failed to stop training {training_id}: {e}")
                return {
                    'success': False,
                    'message': f'åœæ­¢è®­ç»ƒå¤±è´¥: {e}'
                }
    
    def get_training_status(self, training_id: str) -> Optional[Dict[str, Any]]:
        """è·å–è®­ç»ƒçŠ¶æ€"""
        with self.lock:
            if training_id not in self.active_trainings:
                return None
            
            training = self.active_trainings[training_id]
            process = training['process']
            
            # æ›´æ–°è¿›ç¨‹çŠ¶æ€
            if process.poll() is not None and training['status'] == 'running':
                if process.returncode == 0:
                    training['status'] = 'completed'
                else:
                    training['status'] = 'failed'
                training['end_time'] = time.time()
            
            # è¯»å–æœ€æ–°æ—¥å¿—
            logs, new_position = self._read_log_tail(
                training['log_file'], 
                training['last_log_position']
            )
            training['last_log_position'] = new_position
            
            return {
                'training_id': training_id,
                'status': training['status'],
                'pid': training['pid'],
                'start_time': training['start_time'],
                'end_time': training.get('end_time'),
                'config': training['config'],
                'logs': logs,
                'log_file': training['log_file'],  # æ·»åŠ æ—¥å¿—æ–‡ä»¶è·¯å¾„
                'return_code': process.returncode if process.poll() is not None else None
            }
    
    def _read_log_tail(self, log_file: str, last_position: int = 0) -> tuple[List[str], int]:
        """è¯»å–æ—¥å¿—æ–‡ä»¶çš„æ–°å¢å†…å®¹"""
        try:
            if not os.path.exists(log_file):
                return [], last_position
            
            with open(log_file, 'r', encoding='utf-8') as f:
                f.seek(last_position)
                new_lines = f.readlines()
                new_position = f.tell()
                
            return new_lines, new_position
            
        except Exception as e:
            logger.error(f"Failed to read log file {log_file}: {e}")
            return [], last_position
    
    def get_all_trainings(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰è®­ç»ƒä»»åŠ¡çŠ¶æ€"""
        with self.lock:
            result = []
            for training_id, training in self.active_trainings.items():
                status = self.get_training_status(training_id)
                if status:
                    result.append(status)
            return result
    
    def delete_training(self, training_id: str) -> Dict[str, Any]:
        """åˆ é™¤è®­ç»ƒä»»åŠ¡è®°å½•"""
        with self.lock:
            if training_id not in self.active_trainings:
                return {
                    'success': False,
                    'message': f'è®­ç»ƒä»»åŠ¡ {training_id} ä¸å­˜åœ¨'
                }
            
            training = self.active_trainings[training_id]
            
            # å¦‚æœè®­ç»ƒä»åœ¨è¿è¡Œï¼Œå…ˆåœæ­¢
            if training['status'] == 'running':
                stop_result = self.stop_training(training_id)
                if not stop_result['success']:
                    return stop_result
            
            # åˆ é™¤æ—¥å¿—æ–‡ä»¶
            try:
                log_file = Path(training['log_file'])
                if log_file.exists():
                    log_file.unlink()
            except Exception as e:
                logger.warning(f"Failed to delete log file: {e}")
            
            # ä»æ´»åŠ¨è®­ç»ƒä¸­ç§»é™¤
            del self.active_trainings[training_id]
            
            return {
                'success': True,
                'message': f'è®­ç»ƒä»»åŠ¡ {training_id} å·²åˆ é™¤'
            }

# å…¨å±€è®­ç»ƒç®¡ç†å™¨å®ä¾‹
training_manager = TrainingManager()