#!/usr/bin/env python3
"""
åŸºäºSilero VADçš„éŸ³é¢‘åˆ‡åˆ†å¤„ç†è„šæœ¬
åŠŸèƒ½ï¼šä½¿ç”¨VADå¯¹éŸ³é¢‘è¿›è¡Œæ™ºèƒ½åˆ‡åˆ†ï¼ŒåŒæ—¶å¯¹è¿‡çŸ­çš„ç‰‡æ®µè¿›è¡Œåˆå¹¶
"""

import argparse
import os
import time
import torch
import torchaudio
import numpy as np
from pathlib import Path
import warnings
from tqdm import tqdm
from silero_vad import load_silero_vad, get_speech_timestamps
warnings.filterwarnings('ignore')


class VADProcessor:
    def __init__(self, sample_rate=16000, merge_threshold=0.5, split_threshold=10.0):
        """
        åˆå§‹åŒ–VADå¤„ç†å™¨
        
        Args:
            sample_rate: éŸ³é¢‘é‡‡æ ·ç‡
            merge_threshold: æœ€å°éŸ³é¢‘é•¿åº¦é˜ˆå€¼(ç§’)ï¼Œå°äºæ­¤å€¼çš„ç‰‡æ®µä¼šåˆå¹¶
            split_threshold: æœ€å¤§éŸ³é¢‘é•¿åº¦é˜ˆå€¼(ç§’)ï¼Œè¶…è¿‡æ­¤å€¼çš„éŸ³é¢‘ä¼šè¢«åˆ‡åˆ†
        """
        self.sample_rate = sample_rate
        self.merge_threshold = merge_threshold
        self.split_threshold = split_threshold
        
        print("æ­£åœ¨åŠ è½½Silero VADæ¨¡å‹...")
        try:
            self.model = load_silero_vad()
            print("âœ“ VADæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âœ— VADæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
        
    def load_audio(self, file_path):
        """åŠ è½½éŸ³é¢‘æ–‡ä»¶"""
        try:
            waveform, sr = torchaudio.load(file_path)
            
            # é‡é‡‡æ ·åˆ°æŒ‡å®šé‡‡æ ·ç‡
            if sr != self.sample_rate:
                resampler = torchaudio.transforms.Resample(sr, self.sample_rate)
                waveform = resampler(waveform)
            
            # è½¬æ¢ä¸ºå•å£°é“
            if waveform.shape[0] > 1:
                waveform = torch.mean(waveform, dim=0, keepdim=True)
            
            return waveform.squeeze()
        except Exception as e:
            raise RuntimeError(f"åŠ è½½éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {e}")
    
    def save_audio(self, waveform, output_path):
        """ä¿å­˜éŸ³é¢‘æ–‡ä»¶"""
        try:
            # ç¡®ä¿æ³¢å½¢æ•°æ®æ˜¯2ç»´çš„ [channels, samples]
            if waveform.dim() == 1:
                waveform = waveform.unsqueeze(0)
            
            torchaudio.save(
                output_path,
                waveform,
                sample_rate=self.sample_rate,
                encoding="PCM_S",
                bits_per_sample=16
            )
        except Exception as e:
            raise RuntimeError(f"ä¿å­˜éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {e}")
    
    def get_speech_timestamps(self, audio, threshold=0.5, min_speech_duration_ms=250, 
                             min_silence_duration_ms=100, speech_pad_ms=30):
        """è·å–è¯­éŸ³æ—¶é—´æˆ³"""
        timestamps = get_speech_timestamps(
            audio,
            self.model,
            threshold=threshold,
            min_speech_duration_ms=min_speech_duration_ms,
            min_silence_duration_ms=min_silence_duration_ms,
            sampling_rate=self.sample_rate,
            speech_pad_ms=speech_pad_ms
        )
        return timestamps
    
    def merge_short_segments(self, segments, threshold):
        """åˆå¹¶çŸ­éŸ³é¢‘ç‰‡æ®µ"""
        if not segments:
            return []
        
        merged = []
        current_segment = segments[0]
        
        for segment in segments[1:]:
            segment_duration = (segment['end'] - segment['start']) / self.sample_rate
            
            if segment_duration < threshold:
                # å¦‚æœå½“å‰ç‰‡æ®µå¾ˆçŸ­ï¼Œåˆå¹¶åˆ°å‰ä¸€ä¸ªæˆ–åä¸€ä¸ªç‰‡æ®µ
                merged_duration = (current_segment['end'] - current_segment['start']) / self.sample_rate
                
                # å¦‚æœå‰ä¸€ä¸ªç‰‡æ®µä¹ŸçŸ­æˆ–ä¸¤è€…åˆå¹¶åä¸é•¿ï¼Œå°±åˆå¹¶
                if merged_duration + segment_duration < self.split_threshold * 1.5:
                    current_segment['end'] = segment['end']
                else:
                    # å¦åˆ™ç›´æ¥æ·»åŠ åˆ°åˆå¹¶åˆ—è¡¨ï¼Œå¼€å§‹æ–°ç‰‡æ®µ
                    merged.append(current_segment)
                    current_segment = segment
            else:
                # ç‰‡æ®µå¤Ÿé•¿ï¼Œç›´æ¥æ·»åŠ å‰ä¸€ä¸ªç‰‡æ®µ
                merged.append(current_segment)
                current_segment = segment
        
        # æ·»åŠ æœ€åä¸€ä¸ªç‰‡æ®µ
        if current_segment:
            # æ£€æŸ¥æœ€åä¸€ä¸ªç‰‡æ®µæ˜¯å¦éœ€è¦åˆå¹¶
            final_duration = (current_segment['end'] - current_segment['start']) / self.sample_rate
            if final_duration < threshold and merged:
                # åˆå¹¶åˆ°å‰ä¸€ä¸ªç‰‡æ®µ
                merged[-1]['end'] = current_segment['end']
            else:
                merged.append(current_segment)
        
        return merged
    
    def process_audio(self, input_file, output_dir, file_prefix=None):
        """
        å¤„ç†å•ä¸ªéŸ³é¢‘æ–‡ä»¶
        
        Args:
            input_file: è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            file_prefix: è¾“å‡ºæ–‡ä»¶å‰ç¼€ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨åŸæ–‡ä»¶å
        
        Returns:
            åˆ‡åˆ†åçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        try:
            filename = os.path.basename(input_file)
            
            # æ£€æŸ¥è¾“å‡ºç›®å½•
            os.makedirs(output_dir, exist_ok=True)
            
            # åŠ è½½éŸ³é¢‘
            audio = self.load_audio(input_file)
            audio_duration = len(audio) / self.sample_rate
            
            # å¦‚æœéŸ³é¢‘è¾ƒçŸ­ï¼Œç›´æ¥è¿”å›æ— éœ€åˆ‡åˆ†
            if audio_duration <= self.split_threshold:
                if audio_duration < self.merge_threshold:
                    print(f"  è­¦å‘Š: éŸ³é¢‘æ—¶é•¿({audio_duration:.2f}s)å°äºåˆå¹¶é˜ˆå€¼({self.merge_threshold}s)")
                
                output_filename = f"{file_prefix or Path(input_file).stem}.wav"
                output_path = os.path.join(output_dir, output_filename)
                self.save_audio(audio, output_path)
                return [output_path]
            
            # è·å–è¯­éŸ³æ—¶é—´æˆ³
            speech_timestamps = self.get_speech_timestamps(audio)
            
            if not speech_timestamps:
                print(f"  æœªæ£€æµ‹åˆ°è¯­éŸ³ç‰‡æ®µ")
                return []
            
            # åˆå¹¶çŸ­ç‰‡æ®µ
            merged_segments = self.merge_short_segments(speech_timestamps, self.merge_threshold)
            
            # è¿‡æ»¤æœ‰æ•ˆç‰‡æ®µ
            valid_segments = []
            for segment in merged_segments:
                start_sample = int(segment['start'])
                end_sample = int(segment['end'])
                segment_audio = audio[start_sample:end_sample]
                segment_duration = len(segment_audio) / self.sample_rate
                
                if segment_duration >= 0.1:  # ä¿ç•™è‡³å°‘100msçš„ç‰‡æ®µ
                    segment['duration'] = segment_duration
                    valid_segments.append(segment)
                        
            if not valid_segments:
                print(f"  æ²¡æœ‰æœ‰æ•ˆçš„è¯­éŸ³ç‰‡æ®µ")
                return []
            
            # ä¿å­˜åˆ‡åˆ†åçš„éŸ³é¢‘
            output_files = []
            base_name = file_prefix or Path(input_file).stem
            
            for i, segment in enumerate(valid_segments):
                start_sample = int(segment['start'])
                end_sample = int(segment['end'])
                
                segment_audio = audio[start_sample:end_sample]
                output_filename = f"{base_name}_part{i+1:03d}.wav"
                output_path = os.path.join(output_dir, output_filename)
                
                self.save_audio(segment_audio, output_path)
                output_files.append(output_path)
                
            print(f"  ç”Ÿæˆ {len(output_files)} ä¸ªç‰‡æ®µ")
            return output_files
            
        except Exception as e:
            print(f"  å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def process_directory(self, input_dir, output_dir, recursive=False):
        """å¤„ç†æ•´ä¸ªç›®å½•"""
        input_path = Path(input_dir)
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        print(f"æ‰«æç›®å½•: {input_dir}")
        
        # æ”¯æŒçš„éŸ³é¢‘æ ¼å¼
        audio_extensions = {'.wav', '.mp3', '.flac', '.m4a', '.ogg', '.wma'}
        
        # è·å–æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶
        if recursive:
            audio_files = [f for f in input_path.rglob('*') if f.suffix.lower() in audio_extensions]
        else:
            audio_files = [f for f in input_path.iterdir() if f.is_file() and f.suffix.lower() in audio_extensions]
        
        if not audio_files:
            print("æœªæ‰¾åˆ°ä»»ä½•éŸ³é¢‘æ–‡ä»¶")
            return []
        
        print(f"æ‰¾åˆ° {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
        
        all_output_files = []
        
        for audio_file in tqdm(audio_files, desc="å¤„ç†éŸ³é¢‘æ–‡ä»¶"):
            files = self.process_audio(str(audio_file), str(output_path), audio_file.stem)
            all_output_files.extend(files)
        
        print(f"å¤„ç†å®Œæˆï¼Œæ€»å…±ç”Ÿæˆ {len(all_output_files)} ä¸ªæ–‡ä»¶")
        return all_output_files


def main():
    parser = argparse.ArgumentParser(description='ğŸ”Š åŸºäºSilero VADçš„éŸ³é¢‘æ™ºèƒ½åˆ‡åˆ†å·¥å…·')
    parser.add_argument('input', help='è¾“å…¥æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„')
    parser.add_argument('-o', '--output', required=True, help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('-r', '--recursive', action='store_true', 
                       help='é€’å½’å¤„ç†å­ç›®å½•')
    parser.add_argument('--sample-rate', type=int, default=16000,
                       help='è¾“å‡ºé‡‡æ ·ç‡ (é»˜è®¤: 16000)')
    parser.add_argument('--merge-threshold', type=float, default=0.5,
                       help='æœ€å°éŸ³é¢‘é•¿åº¦é˜ˆå€¼(ç§’)ï¼Œå°äºæ­¤å€¼ä¼šè¢«åˆå¹¶ (é»˜è®¤: 0.5)')
    parser.add_argument('--split-threshold', type=float, default=10.0,
                       help='æœ€å¤§éŸ³é¢‘é•¿åº¦é˜ˆå€¼(ç§’)ï¼Œè¶…è¿‡æ­¤å€¼ä¼šè¢«åˆ‡åˆ† (é»˜è®¤: 10.0)')
    
    args = parser.parse_args()
    
    print("ğŸ”Š Silero VAD éŸ³é¢‘åˆ‡åˆ†å·¥å…·")
    print("="*50)
    
    # éªŒè¯è¾“å…¥è·¯å¾„
    if not os.path.exists(args.input):
        print(f"é”™è¯¯: è·¯å¾„ä¸å­˜åœ¨: {args.input}")
        return 1
    
    print(f"è¾“å…¥: {args.input}")
    print(f"è¾“å‡º: {args.output}")
    print(f"é‡‡æ ·ç‡: {args.sample_rate}Hz")
    print(f"åˆ‡åˆ†é˜ˆå€¼: {args.split_threshold}s")
    print(f"åˆå¹¶é˜ˆå€¼: {args.merge_threshold}s")
    
    # åˆ›å»ºVADå¤„ç†å™¨
    try:
        processor = VADProcessor(
            sample_rate=args.sample_rate,
            merge_threshold=args.merge_threshold,
            split_threshold=args.split_threshold
        )
    except Exception as e:
        print(f"åˆå§‹åŒ–å¤±è´¥: {e}")
        return 1
    
    # å¼€å§‹å¤„ç†
    print("="*50)
    
    start_time = time.time()
    total_files = 0
    
    try:
        if os.path.isfile(args.input):
            output_files = processor.process_audio(args.input, args.output)
        elif os.path.isdir(args.input):
            output_files = processor.process_directory(args.input, args.output, args.recursive)
        else:
            print(f"æ— æ•ˆçš„è·¯å¾„ç±»å‹: {args.input}")
            return 1
        
        total_files = len(output_files)
        
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­å¤„ç†")
        return 0
    except Exception as e:
        print(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    elapsed_time = time.time() - start_time
    
    print("="*50)
    print(f"æ€»ç”Ÿæˆæ–‡ä»¶æ•°: {total_files}")
    print(f"æ€»è€—æ—¶: {elapsed_time:.2f}ç§’")
    print("âœ… å¤„ç†å®Œæˆï¼")
    print(f"step 3/5: âœ… All Finished! created {total_files} files -> {args.output}")
    
    return 0


if __name__ == '__main__':
    exit(main())