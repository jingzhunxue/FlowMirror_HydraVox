#!/usr/bin/env python3
"""
Usage:
  python transcribe_to_dataset.py \
      --src data/processed/wav16 \
      --dst data/processed/asr_ds \
      --device gpu \
      --asr_model_path models/SenseVoiceSmall
"""

import argparse
from functools import lru_cache
from pathlib import Path
from typing import List, Dict
import sys

import torch
import torchaudio
import soundfile as sf
from tqdm import tqdm
from datasets import Dataset, Features, Audio, Value
import numpy as np

# ---------- VAD ----------
from silero_vad import load_silero_vad, read_audio, get_speech_timestamps

vad_model = load_silero_vad()

def slice_audio(path: Path, sr: int = 16000, min_sec: float = 0.3):
    wav, sample_rate = torchaudio.load(str(path))
    
    # æ£€æŸ¥é‡‡æ ·ç‡ï¼Œå¦‚æœä¸ä¸€è‡´åˆ™è¿›è¡Œé‡é‡‡æ ·
    if sample_rate != sr:
        print(f"é‡é‡‡æ · {path.name}: {sample_rate}Hz -> {sr}Hz")
        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=sr)
        wav = resampler(wav)
        sample_rate = sr
    
    assert wav.shape[0] == 1 or len(wav.shape) == 1, "only support mono audio"
    ts = get_speech_timestamps(wav.squeeze(), vad_model, threshold=0.5, sampling_rate=sr)
    # åˆå¹¶å¾ˆçŸ­çš„åœé¡¿ç‰‡æ®µ
    merged = []
    for seg in ts:
        if not merged or seg["start"] - merged[-1]["end"] > min_sec * sr:
            merged.append(seg)
        else:
            merged[-1]["end"] = seg["end"]
    # save each chunk to memory buffer (wav bytes)
    chunks = []
    for seg in merged:
        chunk = wav[:, seg["start"]:seg["end"]]
        # ä¿è¯æ˜¯float32ï¼ŒèŒƒå›´[-1, 1]
        buf = chunk.squeeze().cpu().numpy().astype("float32")
        chunks.append(buf.copy())
    return chunks

# ---------- ASR ----------
def load_asr(model_type: str, device: str):
    from modelscope.pipelines import pipeline
    from modelscope.utils.constant import Tasks
    import os
    
    if model_type == "paraformer":
        mdl = pipeline(
            task=Tasks.auto_speech_recognition,
            model='iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch', model_revision="v2.0.4",
            device=device)
    else:
        mdl = pipeline(
            task=Tasks.auto_speech_recognition,
            model='iic/SenseVoiceSmall', model_revision="master",
            device=device)

    return mdl

def asr_transcribe(mdl, wav_buf, sr=16000):
    hyp = mdl(
        input=wav_buf
    )
    # å…¼å®¹è¿”å›listæˆ–dict
    text = ""
    if isinstance(hyp, list):
        if len(hyp) > 0 and isinstance(hyp[0], dict) and "text" in hyp[0]:
            text = hyp[0]["text"]
        else:
            text = ""
    elif isinstance(hyp, dict):
        text = hyp.get("text", "")
    else:
        text = ""
    
    # åå¤„ç†æ–‡æœ¬ï¼šå»é™¤å¤šä½™çš„å­—ç¬¦é—´ç©ºæ ¼ï¼ˆä¿ç•™è¯é—´ç©ºæ ¼ï¼‰
    if text.strip():
        text = post_process_text(text)
    
    return text

def post_process_text(text):
    """
    åå¤„ç†ASRè¾“å‡ºæ–‡æœ¬ï¼Œå»é™¤å¤šä½™ç©ºæ ¼
    """
    import re
    
    # å»é™¤ä¸­æ–‡å­—ç¬¦é—´çš„ç©ºæ ¼ï¼Œä½†ä¿ç•™è‹±æ–‡å•è¯é—´çš„ç©ºæ ¼
    # åŒ¹é…ä¸­æ–‡å­—ç¬¦é—´çš„ç©ºæ ¼
    text = re.sub(r'([\u4e00-\u9fff])\s+([\u4e00-\u9fff])', r'\1\2', text)
    
    # å»é™¤ä¸­æ–‡å­—ç¬¦å’Œæ ‡ç‚¹ç¬¦å·é—´çš„ç©ºæ ¼
    text = re.sub(r'([\u4e00-\u9fff])\s+([ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹])', r'\1\2', text)
    text = re.sub(r'([ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹])\s+([\u4e00-\u9fff])', r'\1\2', text)
    
    # å»é™¤å¤šä½™çš„è¿ç»­ç©ºæ ¼ï¼Œä¿ç•™å•ä¸ªç©ºæ ¼
    text = re.sub(r'\s+', ' ', text)
    
    # å»é™¤é¦–å°¾ç©ºæ ¼
    text = text.strip()
    
    return text

# ---------- å“åº¦æ§åˆ¶ ----------
def normalize_loudness(audio_array, target_loudness_db=-23.0):
    """
    å¯¹éŸ³é¢‘è¿›è¡Œå“åº¦å½’ä¸€åŒ–
    
    Args:
        audio_array: éŸ³é¢‘æ•°ç»„
        target_loudness_db: ç›®æ ‡å“åº¦ (LUFS)
    
    Returns:
        å½’ä¸€åŒ–åçš„éŸ³é¢‘æ•°ç»„
    """
    try:
        import pyloudnorm as pyln
        
        # åˆ›å»ºå“åº¦è®¡é‡å™¨
        meter = pyln.Meter(24000)  # 24kHzé‡‡æ ·ç‡
        
        # æµ‹é‡å½“å‰å“åº¦
        loudness = meter.integrated_loudness(audio_array)
        
        # å¦‚æœæµ‹é‡å¤±è´¥æˆ–éŸ³é¢‘å¤ªå®‰é™ï¼Œä½¿ç”¨ç®€å•çš„RMSå½’ä¸€åŒ–
        if loudness == float('-inf') or np.isnan(loudness):
            return simple_normalize(audio_array)
        
        # è®¡ç®—å½’ä¸€åŒ–å› å­
        loudness_difference = target_loudness_db - loudness
        gain = 10.0 ** (loudness_difference / 20.0)
        
        # åº”ç”¨å¢ç›Š
        normalized_audio = audio_array * gain
        
        # é˜²æ­¢å‰Šæ³¢
        if np.max(np.abs(normalized_audio)) > 0.95:
            normalized_audio = normalized_audio / np.max(np.abs(normalized_audio)) * 0.95
        
        return normalized_audio
        
    except ImportError:
        # å¦‚æœæ²¡æœ‰å®‰è£…pyloudnormï¼Œä½¿ç”¨ç®€å•çš„RMSå½’ä¸€åŒ–
        return simple_normalize(audio_array)
    except Exception:
        # å¦‚æœå“åº¦å½’ä¸€åŒ–å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„RMSå½’ä¸€åŒ–
        return simple_normalize(audio_array)

def simple_normalize(audio_array, target_rms=0.1):
    """
    ç®€å•çš„RMSå½’ä¸€åŒ–
    
    Args:
        audio_array: éŸ³é¢‘æ•°ç»„
        target_rms: ç›®æ ‡RMSå€¼
    
    Returns:
        å½’ä¸€åŒ–åçš„éŸ³é¢‘æ•°ç»„
    """
    if len(audio_array) == 0:
        return audio_array
        
    # è®¡ç®—å½“å‰RMS
    rms = np.sqrt(np.mean(audio_array ** 2))
    
    if rms == 0:
        return audio_array
    
    # è®¡ç®—å¢ç›Š
    gain = target_rms / rms
    
    # åº”ç”¨å¢ç›Š
    normalized_audio = audio_array * gain
    
    # é˜²æ­¢å‰Šæ³¢
    if np.max(np.abs(normalized_audio)) > 0.95:
        normalized_audio = normalized_audio / np.max(np.abs(normalized_audio)) * 0.95
    
    return normalized_audio

# ---------- main ----------
def process_file(path: Path, asr_mdl, sr=16000) -> List[Dict]:
    records = []
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨åŒåçš„txtæ–‡ä»¶
    txt_path = path.with_suffix('.txt')
    if txt_path.exists():
        try:
            # ä»txtæ–‡ä»¶è¯»å–æ–‡æœ¬
            with open(txt_path, 'r', encoding='utf-8') as f:
                text = f.read().strip()
            
            if text:
                # åŠ è½½æ•´ä¸ªéŸ³é¢‘æ–‡ä»¶ï¼ˆä¸è¿›è¡ŒVADåˆ†å‰²ï¼‰
                wav, sample_rate = torchaudio.load(str(path))
                
                # æ£€æŸ¥é‡‡æ ·ç‡ï¼Œå¦‚æœä¸ä¸€è‡´åˆ™è¿›è¡Œé‡é‡‡æ ·
                if sample_rate != sr:
                    print(f"é‡é‡‡æ · {path.name}: {sample_rate}Hz -> {sr}Hz")
                    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=sr)
                    wav = resampler(wav)
                    sample_rate = sr
                
                assert wav.shape[0] == 1 or len(wav.shape) == 1, "only support mono audio"
                
                buf = wav.squeeze().cpu().numpy().astype("float32")
                records.append({"audio": {"array": buf, "sampling_rate": sr}, "text": text})
                return records
        except Exception as e:
            print(f" ! è¯»å–txtæ–‡ä»¶å¤±è´¥ {txt_path}: {e}, ä½¿ç”¨ASRè½¬å½•")
    
    # å¦‚æœæ²¡æœ‰txtæ–‡ä»¶æˆ–è¯»å–å¤±è´¥ï¼Œä½¿ç”¨ASRè½¬å½•
    for buf in slice_audio(path, sr):
        text = asr_transcribe(asr_mdl, buf)
        if text.strip():
            records.append({"audio": {"array": buf, "sampling_rate": sr}, "text": text})
    return records

# ---------- å¤šè¿›ç¨‹å¤„ç† ----------
def worker_process(worker_id, file_chunk, device, gpu_id, min_sec, return_dict):
    """
    å·¥ä½œè¿›ç¨‹å‡½æ•°ï¼Œå¤„ç†åˆ†é…ç»™å®ƒçš„æ–‡ä»¶
    
    Args:
        worker_id: å·¥ä½œè¿›ç¨‹ID
        file_chunk: åˆ†é…ç»™è¯¥è¿›ç¨‹çš„æ–‡ä»¶åˆ—è¡¨
        device: è®¾å¤‡ç±»å‹ (cpu/cuda)
        gpu_id: GPUè®¾å¤‡ID (ä»…åœ¨deviceä¸ºcudaæ—¶æœ‰æ•ˆ)
        min_sec: æœ€å°åˆ†æ®µæ—¶é•¿
        return_dict: ç”¨äºè¿”å›ç»“æœçš„å…±äº«å­—å…¸
    """
    try:
        import os
        import torch
        
        # è®¾ç½®GPUè®¾å¤‡
        if device == "cuda" and gpu_id is not None:
            # è®¾ç½®ç¯å¢ƒå˜é‡
            os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)
            # ç¡®ä¿CUDAå¯ç”¨
            if torch.cuda.is_available():
                target_device = "cuda:0"  # åœ¨è®¾ç½®äº†CUDA_VISIBLE_DEVICESåï¼Œå¯è§çš„GPUæ€»æ˜¯0
                asr_model_type = "paraformer"
                print(f"[Worker {worker_id}] ä½¿ç”¨GPU {gpu_id}ï¼Œæ˜ å°„ä¸º {target_device}")
            else:
                print(f"[Worker {worker_id}] GPU {gpu_id} ä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°CPU")
                target_device = "cpu"
                asr_model_type = "sensevoice"
        else:
            target_device = "cpu"
            asr_model_type = "sensevoice"
        
        print(f"[Worker {worker_id}] å¼€å§‹å¤„ç† {len(file_chunk)} ä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨è®¾å¤‡: {target_device}")
        
        # åŠ è½½ASRæ¨¡å‹
        try:
            asr_mdl = load_asr(asr_model_type, target_device)
            print(f"[Worker {worker_id}] ASRæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"[Worker {worker_id}] ASRæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # å¦‚æœGPUæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°è¯•CPU
            if target_device != "cpu":
                print(f"[Worker {worker_id}] å°è¯•ä½¿ç”¨CPUåŠ è½½æ¨¡å‹")
                target_device = "cpu"
                asr_model_type = "sensevoice"
                asr_mdl = load_asr(asr_model_type, target_device)
            else:
                raise e
        
        # å¤„ç†åˆ†é…çš„æ–‡ä»¶
        worker_records = []
        for fp in tqdm(file_chunk, desc=f"Worker {worker_id}", position=worker_id):
            try:
                recs = process_file(fp, asr_mdl, sr=16000)
                worker_records.extend(recs)
            except Exception as e:
                print(f"[Worker {worker_id}] è·³è¿‡æ–‡ä»¶ {fp.name}: {e}")
        
        return_dict[worker_id] = worker_records
        print(f"[Worker {worker_id}] å®Œæˆå¤„ç†ï¼Œç”Ÿæˆ {len(worker_records)} æ¡è®°å½•")
        
    except Exception as e:
        print(f"[Worker {worker_id}] å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return_dict[worker_id] = []

def process_files_multiprocess(audio_files, device, gpu_devices, num_workers, min_sec):
    """
    å¤šè¿›ç¨‹å¤„ç†éŸ³é¢‘æ–‡ä»¶
    
    Args:
        audio_files: éŸ³é¢‘æ–‡ä»¶åˆ—è¡¨
        device: è®¾å¤‡ç±»å‹
        gpu_devices: GPUè®¾å¤‡åˆ—è¡¨
        num_workers: å·¥ä½œè¿›ç¨‹æ•°
        min_sec: æœ€å°åˆ†æ®µæ—¶é•¿
        
    Returns:
        æ‰€æœ‰è®°å½•çš„åˆ—è¡¨
    """
    from multiprocessing import Process, Manager
    import multiprocessing as mp
    import math
    
    # è®¾ç½®å¯åŠ¨æ–¹æ³•ä¸ºspawnä»¥æ”¯æŒCUDA
    try:
        mp.set_start_method('spawn', force=True)
    except RuntimeError:
        # å¦‚æœå·²ç»è®¾ç½®è¿‡äº†ï¼Œå¿½ç•¥é”™è¯¯
        pass
    
    print(f"ğŸš€ å¯åŠ¨å¤šè¿›ç¨‹å¤„ç†: {num_workers} ä¸ªå·¥ä½œè¿›ç¨‹å¤„ç† {len(audio_files)} ä¸ªæ–‡ä»¶")
    
    # å°†æ–‡ä»¶å¹³å‡åˆ†é…ç»™å„ä¸ªå·¥ä½œè¿›ç¨‹
    chunk_size = math.ceil(len(audio_files) / num_workers)
    file_chunks = [audio_files[i:i + chunk_size] for i in range(0, len(audio_files), chunk_size)]
    
    # åˆ›å»ºå…±äº«å­—å…¸ç”¨äºæ”¶é›†ç»“æœ
    manager = Manager()
    return_dict = manager.dict()
    
    # åˆ›å»ºå¹¶å¯åŠ¨å·¥ä½œè¿›ç¨‹
    processes = []
    for i in range(len(file_chunks)):
        # ä¸ºæ¯ä¸ªè¿›ç¨‹åˆ†é…GPUè®¾å¤‡
        if device == "cuda" and gpu_devices:
            gpu_id = gpu_devices[i % len(gpu_devices)]  # å¾ªç¯åˆ†é…GPU
        else:
            gpu_id = None
        
        p = Process(
            target=worker_process,
            args=(i, file_chunks[i], device, gpu_id, min_sec, return_dict)
        )
        processes.append(p)
        p.start()
        print(f"[ä¸»è¿›ç¨‹] å¯åŠ¨å·¥ä½œè¿›ç¨‹ {i}ï¼Œåˆ†é… {len(file_chunks[i])} ä¸ªæ–‡ä»¶ï¼ŒGPU: {gpu_id}")
    
    # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
    for i, p in enumerate(processes):
        p.join()
        print(f"[ä¸»è¿›ç¨‹] å·¥ä½œè¿›ç¨‹ {i} å·²å®Œæˆ")
    
    # åˆå¹¶æ‰€æœ‰ç»“æœ
    all_records = []
    for worker_id in sorted(return_dict.keys()):
        worker_records = return_dict[worker_id]
        all_records.extend(worker_records)
        print(f"[ä¸»è¿›ç¨‹] åˆå¹¶å·¥ä½œè¿›ç¨‹ {worker_id} çš„ {len(worker_records)} æ¡è®°å½•")
    
    print(f"âœ… å¤šè¿›ç¨‹å¤„ç†å®Œæˆï¼Œæ€»å…±ç”Ÿæˆ {len(all_records)} æ¡è®°å½•")
    return all_records

def build_dataset(records, dst: Path, batch_size: int = 1000):
    """
    åˆ†æ‰¹å¤„ç†æ•°æ®é›†ï¼Œé¿å…å†…å­˜æº¢å‡º
    
    Args:
        records: éŸ³é¢‘è®°å½•åˆ—è¡¨
        dst: è¾“å‡ºç›®å½•
        batch_size: æ‰¹å¤„ç†å¤§å°
    """
    total_records = len(records)
    print(f"æ€»è®°å½•æ•°: {total_records}")
    
    if total_records == 0:
        print("âš ï¸ æ²¡æœ‰è®°å½•å¯å¤„ç†")
        return
    
    # å¦‚æœè®°å½•æ•°è¾ƒå°‘ï¼Œç›´æ¥å¤„ç†
    if total_records <= batch_size:
        print("è®°å½•æ•°è¾ƒå°‘ï¼Œç›´æ¥å¤„ç†...")
        # å¯¹æ¯ä¸ªaudioè¿›è¡Œå“åº¦æ§åˆ¶
        print("æ­£åœ¨è¿›è¡Œå“åº¦æ§åˆ¶...")
        for i, record in enumerate(tqdm(records, desc="Normalizing")):
            audio_array = record["audio"]["array"]
            normalized_audio = normalize_loudness(audio_array)
            records[i]["audio"]["array"] = normalized_audio
        
        print("å¼€å§‹ç”ŸæˆDataset...")
        features = Features({"audio": Audio(sampling_rate=16000), "text": Value("string")})
        ds = Dataset.from_list(records, features=features)
        ds.save_to_disk(dst)
        print(f"âœ“ Saved dataset with {len(ds)} records -> {dst}")
        return
    
    # åˆ†æ‰¹å¤„ç†å¤§æ•°æ®é›†
    print(f"å¼€å§‹åˆ†æ‰¹å¤„ç†ï¼Œæ‰¹å¤§å°: {batch_size}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    dst.mkdir(parents=True, exist_ok=True)
    
    # åˆ†æ‰¹å¤„ç†
    num_batches = (total_records + batch_size - 1) // batch_size
    
    all_datasets = []
    
    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min((batch_idx + 1) * batch_size, total_records)
        
        print(f"\nå¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{num_batches} (è®°å½• {start_idx}-{end_idx})")
        
        # è·å–å½“å‰æ‰¹æ¬¡çš„è®°å½•
        batch_records = records[start_idx:end_idx]
        
        # å“åº¦æ§åˆ¶
        print("æ­£åœ¨è¿›è¡Œå“åº¦æ§åˆ¶...")
        for i, record in enumerate(tqdm(batch_records, desc=f"Normalizing batch {batch_idx + 1}")):
            audio_array = record["audio"]["array"]
            normalized_audio = normalize_loudness(audio_array)
            batch_records[i]["audio"]["array"] = normalized_audio
        
        # åˆ›å»ºå½“å‰æ‰¹æ¬¡çš„dataset
        print(f"åˆ›å»ºæ‰¹æ¬¡ {batch_idx + 1} çš„Dataset...")
        features = Features({"audio": Audio(sampling_rate=16000), "text": Value("string")})
        batch_ds = Dataset.from_list(batch_records, features=features)
        
        # ä¿å­˜å½“å‰æ‰¹æ¬¡
        batch_path = dst / f"batch_{batch_idx:04d}"
        batch_ds.save_to_disk(batch_path)
        
        all_datasets.append(batch_ds)
        
        print(f"âœ“ æ‰¹æ¬¡ {batch_idx + 1} å·²ä¿å­˜åˆ° {batch_path}")
        
        # æ¸…ç†å†…å­˜
        del batch_records
        del batch_ds
        import gc
        gc.collect()
    
    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
    print(f"\nåˆå¹¶ {len(all_datasets)} ä¸ªæ‰¹æ¬¡...")
    try:
        from datasets import concatenate_datasets
        final_ds = concatenate_datasets(all_datasets)
        
        # ä¿å­˜æœ€ç»ˆdataset
        final_path = dst / "final_dataset"
        final_ds.save_to_disk(final_path)
        
        print(f"âœ“ æœ€ç»ˆæ•°æ®é›†å·²ä¿å­˜åˆ° {final_path}")
        print(f"æ€»è®°å½•æ•°: {len(final_ds)}")
        
        # æ¸…ç†æ‰¹æ¬¡æ–‡ä»¶
        import shutil
        for batch_idx in range(num_batches):
            batch_path = dst / f"batch_{batch_idx:04d}"
            if batch_path.exists():
                shutil.rmtree(batch_path)
        
        print("âœ“ å·²æ¸…ç†ä¸´æ—¶æ‰¹æ¬¡æ–‡ä»¶")
        
    except Exception as e:
        print(f"âš ï¸ åˆå¹¶å¤±è´¥: {e}")
        print(f"æ‰¹æ¬¡æ–‡ä»¶ä¿å­˜åœ¨: {dst}")
        print("ä½ å¯ä»¥æ‰‹åŠ¨åŠ è½½å„ä¸ªæ‰¹æ¬¡æ–‡ä»¶")
    
    finally:
        # æ¸…ç†å†…å­˜
        del all_datasets
        import gc
        gc.collect()

def main():
    # è®¾ç½®multiprocessingå¯åŠ¨æ–¹æ³•ä¸ºspawnä»¥æ”¯æŒCUDA
    import multiprocessing as mp
    try:
        mp.set_start_method('spawn', force=True)
    except RuntimeError:
        # å¦‚æœå·²ç»è®¾ç½®è¿‡äº†ï¼Œå¿½ç•¥é”™è¯¯
        pass
    
    ap = argparse.ArgumentParser()
    ap.add_argument("--src", type=Path, required=True, help="éŸ³é¢‘æ–‡ä»¶æ ¹ç›®å½•")
    ap.add_argument("--dst", type=Path, required=True, help="è¾“å‡º datasets ç›®å½•")
    ap.add_argument("--device", choices=["cpu", "cuda"], default="cpu")
    ap.add_argument("--gpu_devices", type=str, default="", help="æŒ‡å®šGPUè®¾å¤‡ï¼Œç”¨é€—å·åˆ†éš”ï¼Œå¦‚: 0,1,2,3")
    ap.add_argument("--num_workers", type=int, default=1, help="å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°")
    ap.add_argument("--min_sec", type=float, default=0.3, help="åˆ†æ®µæœ€å°é—´éš” (s)")
    ap.add_argument("--batch_size", type=int, default=1000, help="æ‰¹å¤„ç†å¤§å°ï¼Œé¿å…å†…å­˜æº¢å‡º (é»˜è®¤: 1000)")
    args = ap.parse_args()

    args.dst.parent.mkdir(parents=True, exist_ok=True)

    # å¤„ç†GPUè®¾å¤‡é…ç½®
    gpu_devices = []
    if args.device == "cuda" and torch.cuda.is_available():
        if args.gpu_devices.strip():
            # è§£ææŒ‡å®šçš„GPUè®¾å¤‡
            gpu_devices = [int(d.strip()) for d in args.gpu_devices.split(',') if d.strip().isdigit()]
            # éªŒè¯GPUè®¾å¤‡æ˜¯å¦æœ‰æ•ˆ
            available_gpus = list(range(torch.cuda.device_count()))
            gpu_devices = [d for d in gpu_devices if d in available_gpus]
        else:
            # ä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU
            gpu_devices = list(range(torch.cuda.device_count()))
        
        if not gpu_devices:
            print("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„GPUè®¾å¤‡ï¼Œä½¿ç”¨CPU")
            device = "cpu"
        else:
            device = "cuda"
            print(f"ğŸš€ å°†ä½¿ç”¨GPUè®¾å¤‡: {gpu_devices}")
    else:
        device = "cpu"
        print("ğŸ–¥ï¸ ä½¿ç”¨CPUè®¾å¤‡")
    
    # è°ƒæ•´å·¥ä½œè¿›ç¨‹æ•°
    if device == "cuda" and len(gpu_devices) > 1:
        # å¤šGPUæƒ…å†µä¸‹ï¼Œæ¯ä¸ªGPUä¸€ä¸ªè¿›ç¨‹
        args.num_workers = min(args.num_workers, len(gpu_devices))
        print(f"ğŸ“Š å¤šGPUå¹¶è¡Œå¤„ç†ï¼Œä½¿ç”¨ {args.num_workers} ä¸ªå·¥ä½œè¿›ç¨‹")
    elif device == "cpu":
        # CPUæƒ…å†µä¸‹é™åˆ¶è¿›ç¨‹æ•°
        import os
        args.num_workers = min(args.num_workers, os.cpu_count())
        print(f"ğŸ”§ CPUå¹¶è¡Œå¤„ç†ï¼Œä½¿ç”¨ {args.num_workers} ä¸ªå·¥ä½œè¿›ç¨‹")

    # åŒæ—¶æŸ¥æ‰¾ .wav å’Œ .mp3 æ–‡ä»¶
    wav_files = sorted(args.src.rglob("*.wav"))
    mp3_files = sorted(args.src.rglob("*.mp3"))
    audio_files = wav_files + mp3_files
    
    if not audio_files:
        print(f"é”™è¯¯ï¼šåœ¨ç›®å½• '{args.src}' ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½• .wav æˆ– .mp3 æ–‡ä»¶ã€‚")
        sys.exit(1)
    
    print(f"æ‰¾åˆ° {len(wav_files)} ä¸ª .wav æ–‡ä»¶å’Œ {len(mp3_files)} ä¸ª .mp3 æ–‡ä»¶")
    
    # å¤šè¿›ç¨‹å¤„ç†
    if args.num_workers > 1:
        all_records = process_files_multiprocess(
            audio_files, device, gpu_devices, 
            args.num_workers, args.min_sec
        )
    else:
        # å•è¿›ç¨‹å¤„ç†ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        asr_model_type = "paraformer" if device == "cuda" else "sensevoice"
        target_device = f"cuda:{gpu_devices[0]}" if device == "cuda" and gpu_devices else device
        print('Loading ASR model...')
        asr_mdl = load_asr(asr_model_type, target_device)
        print(f"[ASR] using {asr_model_type} on {target_device}")
        
        all_records = []
        for fp in tqdm(audio_files, desc="ASR"):
            recs = process_file(fp, asr_mdl, sr=16000)
            all_records.extend(recs)

    if not all_records:
        print("é”™è¯¯ï¼šæœªèƒ½ä»éŸ³é¢‘æ–‡ä»¶ä¸­æå–ä»»ä½•æœ‰æ•ˆçš„è¯­éŸ³æ–‡æœ¬å¯¹ã€‚")
        print("è¯·æ£€æŸ¥æ‚¨çš„éŸ³é¢‘æ–‡ä»¶æ˜¯å¦åŒ…å«æ¸…æ™°çš„è¯­éŸ³ï¼Œæˆ–å°è¯•è°ƒæ•´VADå‚æ•°ã€‚")
        sys.exit(1)

    build_dataset(all_records, args.dst, args.batch_size)
    print(f"step 4/5: âœ… All Finished! Transcribed {len(all_records)} files -> {args.dst}")

if __name__ == "__main__":
    main()
