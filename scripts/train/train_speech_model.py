# -*- coding: utf-8 -*-
"""
HydraVoxè¯­éŸ³æ¨¡å‹è®­ç»ƒè„šæœ¬
ä½¿ç”¨Hugging Face Trainerè¿›è¡ŒLLMå’ŒFLOWæ¨¡å‹çš„è®­ç»ƒ
æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒã€æ··åˆç²¾åº¦ã€DeepSpeedç­‰åŠŸèƒ½
"""
from __future__ import annotations

import argparse
import logging
import os
import sys
from pathlib import Path
from typing import Dict, List, Union, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent.absolute()
third_party_dir = project_root / "server/model_utils"

sys.path.insert(0, str(project_root))
sys.path.insert(0, str(third_party_dir))

import numpy as np
import torch
import torchaudio
from datasets import load_from_disk, concatenate_datasets, Dataset, Audio
from hyperpyyaml import load_hyperpyyaml
from torch.nn.utils.rnn import pad_sequence
from transformers import (
    Trainer,
    TrainingArguments,
    PreTrainedTokenizerBase,
)

# ---------- Domain-specific imports ---------- #
from server.model_utils.cosyvoice.tokenizer.tokenizer import get_qwen_tokenizer  
from server.model_utils.matcha.utils.audio import mel_spectrogram
from modelscope.pipelines import pipeline
import accelerate
import onnxruntime as ort
import whisper
import multiprocessing as mp

mp.set_start_method('spawn', force=True)

from fmtn import create_default_tn


tn = create_default_tn(verbose=True)

so = ort.SessionOptions()
so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
so.intra_op_num_threads = 1

accelerator = accelerate.Accelerator()

rank_id = accelerator.process_index

print(f"here once, rank_id: {rank_id}")

EMBEDDING_MODEL_PATH="jzx-ai-lab/speech_campplus_sv_zh-cn_16k-common"
TOKENIZER_ONNX_PATH = Path(
    "jzx-ai-lab/HydraVox/speech_tokenizer_v2.onnx"
).expanduser().resolve()
providers = (
    [("CUDAExecutionProvider", {"device_id": rank_id})]
)
# -----------------------------------------------------------------------------
# Data preparation helpers
# -----------------------------------------------------------------------------

USEFUL_COLUMNS_LLM = ["text", "text_token", "speech_token", "audio"]
USEFUL_COLUMNS_FLOW = ["speech_token", "audio", "embedding"]


sv_pipe = pipeline(
    task="speaker-verification",
    model=EMBEDDING_MODEL_PATH,
    model_revision="v1.0.0",
    device=f"cuda:{rank_id}",  # ä½¿ç”¨å…·ä½“çš„è®¾å¤‡ ID
)

tokenizer_session = ort.InferenceSession(TOKENIZER_ONNX_PATH.as_posix(),
                                 sess_options=so,
                                 providers=providers)

tokenizer = get_qwen_tokenizer(
    token_path="jzx-ai-lab/HydraVox/CosyVoice-BlankEN", skip_special_tokens=True
)

def prepare_dataset_llm(ds: Dataset, tokenizer: PreTrainedTokenizerBase) -> Dataset:
    """ä¿ç•™LLMè®­ç»ƒæ‰€éœ€çš„åˆ—"""
    ds = ds.remove_columns([c for c in ds.column_names if c not in USEFUL_COLUMNS_LLM])
    ds = ds.cast_column("audio", Audio(decode=True, sampling_rate=16000))
    return ds


def _load_audio_with_fallback(audio_info, target_sr: int | None = None, neighbor_list=None, idx: int = None, mono: bool = True):
    """åŠ è½½éŸ³é¢‘ï¼Œæ”¯æŒé‚»è¿‘æ ·æœ¬å›é€€"""
    def load_single_audio(info):
        if isinstance(info, dict):
            if "array" in info:
                wav = torch.tensor(info["array"], dtype=torch.float32)
                if mono and wav.dim() > 1:
                    wav = wav.mean(dim=0, keepdim=True)
                else:
                    wav = wav.unsqueeze(0)
                if target_sr is not None and info["sampling_rate"] != target_sr:
                    wav = torchaudio.transforms.Resample(info["sampling_rate"], target_sr)(wav)
                return wav
            path = "/home/ecs-user/nas_training_data/HanxueTTS/downloaded_audio/" + info.get("path")
        elif isinstance(info, str):
            path = info
        else:
            raise ValueError(f"Invalid audio info: {info}")
        
        if not os.path.exists(path):
            raise FileNotFoundError(f"Audio not found: {path}")
        
        wav, sr = torchaudio.load(path)
        if mono and wav.size(0) > 1:
            wav = wav.mean(dim=0, keepdim=True)
        if target_sr is not None and sr != target_sr:
            wav = torchaudio.transforms.Resample(sr, target_sr)(wav)
        return wav
    
    # å°è¯•åŠ è½½å½“å‰éŸ³é¢‘
    try:
        return load_single_audio(audio_info)
    except Exception as e:
        print("Audio not found: ", audio_info, e)
        pass
    
    # å°è¯•é‚»è¿‘æ ·æœ¬
    if neighbor_list and idx is not None:
        for offset in range(1, min(4, len(neighbor_list))):
            for cand in (idx - offset, idx + offset):
                if 0 <= cand < len(neighbor_list):
                    try:
                        print(f"ä½¿ç”¨é‚»è¿‘æ ·æœ¬ {cand} æ›¿ä»£å¤±è´¥çš„ {idx}")
                        return load_single_audio(neighbor_list[cand])
                    except Exception:
                        continue
    
    # æœ€ç»ˆå›é€€
    print(f"éŸ³é¢‘åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é™éŸ³: {idx}")
    return torch.zeros(1, 1600, dtype=torch.float32)

def process_audio_unified(audio_dict, neighbor_list=None, idx=None, need_speech_feat=True, need_embedding=True, need_speech_token=True):
    """ç»Ÿä¸€éŸ³é¢‘å¤„ç†ï¼šæå–melç‰¹å¾ã€embeddingã€speech token"""
    
    def extract_features(audio_info):
        results = {}
        
        if need_speech_feat:
            wav_24k = _load_audio_with_fallback(audio_info, 24000, neighbor_list, idx)
            # melç‰¹å¾å¤„ç†
            if (wav_24k.shape[-1] - 480) // 480 % 2 == 0:
                wav_24k = torch.nn.functional.pad(wav_24k, (0, 480))
            mel = mel_spectrogram(wav_24k, 1920, 80, 24000, 480, 1920, 0, 8000, False)
            results["speech_feat"] = mel.squeeze(0).transpose(0, 1)
        
        if need_embedding:
            wav_16k = _load_audio_with_fallback(audio_info, 16000, neighbor_list, idx)
            wav_np = wav_16k.squeeze(0).cpu().numpy().astype(np.float32)
            sv_out = sv_pipe([wav_np], output_emb=True)
            results["embedding"] = np.array(sv_out["embs"][0], dtype=np.float32)
        
        if need_speech_token:
            wav_16k = _load_audio_with_fallback(audio_info, 16000, neighbor_list, idx)
            mel_whisper = whisper.log_mel_spectrogram(wav_16k, n_mels=128)
            ort_inputs = {
                tokenizer_session.get_inputs()[0].name: mel_whisper.cpu().numpy(),
                tokenizer_session.get_inputs()[1].name: np.array([mel_whisper.shape[2]], dtype=np.int32),
            }
            tokens = tokenizer_session.run(None, ort_inputs)[0].flatten().tolist()
            results["speech_token"] = tokens
            results["speech_token_len"] = len(tokens)
        
        return results

    try:
        return extract_features(audio_dict)
    except Exception as e:
        print(f"éŸ³é¢‘å¤„ç†å¤±è´¥ {idx}: {e}, ä½¿ç”¨é»˜è®¤å€¼")
        
        # è¿”å›é»˜è®¤å€¼
        results = {}
        if need_speech_feat:
            results["speech_feat"] = torch.zeros(100, 80, dtype=torch.float32)
        if need_embedding:
            results["embedding"] = np.zeros(256, dtype=np.float32)
        if need_speech_token:
            results["speech_token"] = [0] * 50
            results["speech_token_len"] = 50
        return results


def prepare_dataset_flow(ds: Dataset) -> Dataset:
    """ä¿ç•™FLOWè®­ç»ƒæ‰€éœ€çš„åˆ—"""
    ds = ds.remove_columns([c for c in ds.column_names if c not in USEFUL_COLUMNS_FLOW])
    ds = ds.cast_column("audio", Audio(decode=True, sampling_rate=16000))
    return ds


# -----------------------------------------------------------------------------
# Data collator (pads variable-length tensors manually to keep Trainer happy)
# -----------------------------------------------------------------------------

def _process_audio_features(features: List[Dict], need_speech_feat: bool, need_embedding: bool, need_speech_token: bool):
    """å¤„ç†éŸ³é¢‘ç‰¹å¾"""
    neighbor_list = [f["audio"] for f in features]
    batch = {}
    
    speech_feats, speech_feat_lens = [], []
    embeddings = []
    speech_tokens, speech_token_lens = [], []
    
    for i, f in enumerate(features):
        audio_results = process_audio_unified(
            f["audio"], neighbor_list, i,
            need_speech_feat, need_embedding, need_speech_token
        )
        
        if need_speech_feat:
            mel = audio_results["speech_feat"].to(torch.bfloat16)
            speech_feats.append(mel)
            speech_feat_lens.append(len(mel))
        
        if need_embedding:
            emb = torch.tensor(audio_results["embedding"], dtype=torch.bfloat16)
            embeddings.append(emb)
        
        if need_speech_token:
            speech_token = torch.tensor(audio_results["speech_token"], dtype=torch.long)
            speech_tokens.append(speech_token)
            speech_token_lens.append(audio_results["speech_token_len"])
    
    if need_speech_feat:
        batch["speech_feat"] = pad_sequence(speech_feats, batch_first=True)
        batch["speech_feat_len"] = torch.tensor(speech_feat_lens)
    if need_embedding:
        batch["embedding"] = torch.stack(embeddings)
    if need_speech_token:
        batch["speech_token"] = pad_sequence(speech_tokens, batch_first=True)
        batch["speech_token_len"] = torch.tensor(speech_token_lens, dtype=torch.int64)
    
    return batch

def _process_text_features(features: List[Dict]):
    """å¤„ç†æ–‡æœ¬ç‰¹å¾"""
    batch = {}
    
    # å¤„ç†åŸå§‹æ–‡æœ¬
    if "text" in features[0] and "text_token" not in features[0]:
        text_tokens, text_token_lens = [], []
        for f in features:
            try:
                text_tn = tn.process_text(f["text"])
            except Exception:
                text_tn = f["text"]
            text_token = tokenizer.encode(text_tn, allowed_special="all")
            text_tokens.append(torch.tensor(text_token, dtype=torch.long))
            text_token_lens.append(len(text_token))
        batch["text_token"] = pad_sequence(text_tokens, batch_first=True)
        batch["text_token_len"] = torch.tensor(text_token_lens, dtype=torch.int64)
    
    # å¤„ç†é¢„å¤„ç†çš„text_token
    elif "text_token" in features[0]:
        text_tokens, text_token_lens = [], []
        for f in features:
            text_token = f["text_token"]
            if not isinstance(text_token, torch.Tensor):
                text_token = torch.tensor(text_token, dtype=torch.long)
            text_tokens.append(text_token.long())
            text_token_lens.append(len(text_token))
        batch["text_token"] = pad_sequence(text_tokens, batch_first=True)
        batch["text_token_len"] = torch.tensor(text_token_lens)
    
    return batch

def _process_existing_features(features: List[Dict]):
    """å¤„ç†å·²å­˜åœ¨çš„ç‰¹å¾"""
    batch = {}
    
    # å¤„ç†å·²æœ‰çš„speech_token
    if "speech_token" in features[0]:
        speech_tokens, speech_token_lens = [], []
        for f in features:
            speech_token = torch.tensor(f["speech_token"], dtype=torch.long)
            speech_tokens.append(speech_token)
            speech_token_lens.append(len(speech_token))
        batch["speech_token"] = pad_sequence(speech_tokens, batch_first=True)
        batch["speech_token_len"] = torch.tensor(speech_token_lens)
    
    # å¤„ç†å·²æœ‰çš„embedding
    if "embedding" in features[0]:
        embeddings = []
        for f in features:
            emb = torch.tensor(f["embedding"], dtype=torch.bfloat16)
            embeddings.append(emb)
        batch["embedding"] = torch.stack(embeddings)
    
    return batch

class ModelAwareDataCollator:
    """æ ¹æ®æ¨¡å‹ç±»å‹æ™ºèƒ½å¤„ç†ç‰¹å¾çš„æ•°æ®æ‰¹å¤„ç†å™¨"""
    
    def __init__(self, model_type: str):
        self.model_type = model_type
    
    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        """æ•°æ®æ‰¹å¤„ç†å‡½æ•°"""
        batch = {}
        
        # æ ¹æ®æ¨¡å‹ç±»å‹å†³å®šéœ€è¦çš„ç‰¹å¾
        if self.model_type == "llm":
            # LLMæ¨¡å‹åªéœ€è¦speech_tokenï¼Œä¸éœ€è¦speech_featå’Œembedding
            need_speech_feat = False
            need_embedding = False
            need_speech_token = "audio" in features[0] and "speech_token" not in features[0]
        elif self.model_type == "flow":
            # FLOWæ¨¡å‹ä¿æŒå®Œæ•´é€»è¾‘
            need_speech_feat = "audio" in features[0] and "speech_feat" not in features[0]
            need_embedding = "audio" in features[0] and "embedding" not in features[0] and "text_token" not in features[0]
            need_speech_token = "audio" in features[0] and "speech_token" not in features[0]
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")
        
        # å¤„ç†éŸ³é¢‘ç‰¹å¾
        if need_speech_feat or need_embedding or need_speech_token:
            batch.update(_process_audio_features(features, need_speech_feat, need_embedding, need_speech_token))
        
        # å¤„ç†æ–‡æœ¬ç‰¹å¾
        batch.update(_process_text_features(features))
        
        # å¤„ç†å·²å­˜åœ¨çš„ç‰¹å¾
        batch.update(_process_existing_features(features))
        
        return batch

def collate_fn(features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
    """å…¼å®¹æ€§å‡½æ•°ï¼Œé»˜è®¤ä½¿ç”¨FLOWæ¨¡å‹é€»è¾‘"""
    collator = ModelAwareDataCollator("flow")
    return collator(features)


# -----------------------------------------------------------------------------
# ä¸»å‡½æ•°
# -----------------------------------------------------------------------------


def main():
    parser = argparse.ArgumentParser()
    # åŸºæœ¬å‚æ•°
    parser.add_argument("--config", type=str, help="æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model", choices=["llm", "flow"], required=True, help="æ¨¡å‹ç±»å‹")
    parser.add_argument("--model_ckpt", type=str, required=True, help="æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--tokenizer_path", type=str, required=True, help="åˆ†è¯å™¨è·¯å¾„")
    parser.add_argument("--train_data", type=str, required=True, help="è®­ç»ƒæ•°æ®è·¯å¾„ï¼Œé€—å·åˆ†éš”")
    parser.add_argument("--cv_data", type=str, required=False, help="éªŒè¯æ•°æ®è·¯å¾„ï¼Œé€—å·åˆ†éš”")
    parser.add_argument("--output_dir", type=str, required=True, help="è¾“å‡ºç›®å½•")

    # è®­ç»ƒå‚æ•°
    parser.add_argument("--per_device_train_batch_size", type=int, default=1)
    parser.add_argument("--per_device_eval_batch_size", type=int, default=1)
    parser.add_argument("--learning_rate", type=float, default=1e-4)
    parser.add_argument("--num_train_epochs", type=int, default=10)
    parser.add_argument("--fp16", action="store_true", default=False)
    parser.add_argument("--bf16", action="store_true", default=False)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1)
    parser.add_argument("--deepspeed", type=str, default=None, help="DeepSpeedé…ç½®æ–‡ä»¶")

    parser.add_argument("--enable_lora", action="store_true", default=False)
    parser.add_argument("--lora_r", type=int, default=64)
    parser.add_argument("--lora_alpha", type=int, default=128)
    parser.add_argument("--lora_dropout", type=float, default=0.05)
    parser.add_argument("--lora_bias", type=str, default="none")
    parser.add_argument("--lora_target_modules", type=list, default=["q_proj", "v_proj", "k_proj"])

    parser.add_argument("--logging_steps", type=int, default=50)
    parser.add_argument("--eval_steps", type=int, default=1000)
    parser.add_argument("--save_steps", type=int, default=2000)
    parser.add_argument("--save_total_limit", type=int, default=None)
    parser.add_argument("--dataloader_num_workers", type=int, default=8)
    parser.add_argument("--auto_val_split", action="store_true", default=False, help="è‡ªåŠ¨åˆ’åˆ†éªŒè¯é›†")
    parser.add_argument("--val_split_ratio", type=float, default=0.05, help="éªŒè¯é›†æ¯”ä¾‹")

    args, unknown = parser.parse_known_args()

    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
    logging.info("ğŸš€ è®­ç»ƒè„šæœ¬å¯åŠ¨")
    logging.info("æ­£åœ¨åŠ è½½é…ç½®æ–‡ä»¶...")

    # æ„å»ºæ¨¡å‹
    if args.config is None:
        model_dir = os.getenv("TTS_MODEL_DIR", "jzx-ai-lab/HydraVox")
        args.config = os.path.join(model_dir, "hydravox.yaml")
        logging.info(f"ä½¿ç”¨é»˜è®¤é…ç½®æ–‡ä»¶: {args.config}")
    
    with open(args.config, "r") as f:
        model_dir = os.getenv("TTS_MODEL_DIR", "jzx-ai-lab/HydraVox")
        cfg = load_hyperpyyaml(f, overrides={
            'qwen_pretrain_path': os.path.join(model_dir, 'CosyVoice-BlankEN')
        })
    model = cfg[args.model]

    # æ•°æ®å¤„ç†
    try:
        logging.info("Preprocessing datasets â€¦")
        
        # å‚æ•°éªŒè¯ï¼šéè‡ªåŠ¨åˆ’åˆ†æ—¶éœ€è¦cv_data
        if not args.auto_val_split and (not args.cv_data or not args.cv_data.strip()):
            raise ValueError("å½“æœªå¯ç”¨è‡ªåŠ¨åˆ’åˆ†éªŒè¯é›†æ—¶ï¼Œ--cv_data å‚æ•°æ˜¯å¿…éœ€çš„")
            
        train_paths = args.train_data.split(",")
        tokenizer = get_qwen_tokenizer(
            token_path=args.tokenizer_path, skip_special_tokens=True
        ) if args.model == "llm" else None
        model_state = torch.load(args.model_ckpt)
        if args.model == "llm":
            if 'epoch' in model_state:
                model_state.pop('epoch')
            if 'step' in model_state:
                model_state.pop('step')
        model.load_state_dict(model_state)

        if args.auto_val_split:
            # åªåŠ è½½è®­ç»ƒé›†ï¼Œè‡ªåŠ¨åˆ’åˆ†
            train_dss = [load_from_disk(p) for p in train_paths]
            if args.model == "llm":
                train_dss = [prepare_dataset_llm(ds, tokenizer) for ds in train_dss]
            else:
                train_dss = [prepare_dataset_flow(ds) for ds in train_dss]
            full_dataset = concatenate_datasets(train_dss).shuffle(seed=42)
            val_size = int(len(full_dataset) * args.val_split_ratio)
            train_size = len(full_dataset) - val_size
            train_dataset = full_dataset.select(range(train_size))
            eval_dataset = full_dataset.select(range(train_size, train_size + val_size))
            logging.info(f"è‡ªåŠ¨åˆ’åˆ†éªŒè¯é›†: è®­ç»ƒé›† {train_size}ï¼ŒéªŒè¯é›† {val_size}")
        else:
            val_paths = args.cv_data.split(",")
            train_dss = [load_from_disk(p) for p in train_paths]
            val_dss = [load_from_disk(p) for p in val_paths]
            # ç»Ÿä¸€éŸ³é¢‘æ ¼å¼ï¼Œé¿å…concatenate_datasetsæ—¶å‡ºé”™
            if args.model == "flow":
                logging.info("Unifying audio format across datasets for FLOW...")
                # ä»…ä¿ç•™è·¯å¾„ä¸å…ƒä¿¡æ¯ï¼Œé¿å…è¿™é‡Œè§£ç ï¼›åç»­ç”¨ torchaudio æŒ‰è·¯å¾„è¯»å–
                train_dss = [ds for ds in train_dss]
                val_dss = [ds for ds in val_dss]
            if args.model == "llm":
                logging.info("Preparing LLM dataset...")
                train_dss = [prepare_dataset_llm(ds, tokenizer) for ds in train_dss]
                val_dss = [prepare_dataset_llm(ds, tokenizer) for ds in val_dss]

            else:
                raise ValueError("Invalid model type")
            train_dataset = concatenate_datasets(train_dss).shuffle(seed=42).cast_column("audio", Audio(sampling_rate=None, mono=None, decode=False))
            eval_dataset = concatenate_datasets(val_dss).shuffle(seed=42).cast_column("audio", Audio(sampling_rate=None, mono=None, decode=False))
    except Exception as e:
        logging.error(f"âŒError preprocessing datasets: {e}")
        raise e

    # è®­ç»ƒå™¨é…ç½®
    try:
        if args.enable_lora:
            from peft import LoraConfig, get_peft_model
            lora_config = LoraConfig(
                r=args.lora_r,
                lora_alpha=args.lora_alpha,
                target_modules=args.lora_target_modules,
                lora_dropout=args.lora_dropout,
                bias=args.lora_bias,
            )
            model = get_peft_model(model, lora_config)

        logging.info("Initialising Trainer â€¦")
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®ševal_batch_sizeï¼Œä½¿ç”¨train_batch_size
        eval_batch_size = args.per_device_eval_batch_size if args.per_device_eval_batch_size is not None else args.per_device_train_batch_size
        
        training_args = TrainingArguments(
            output_dir=args.output_dir,
            logging_dir=os.path.join(args.output_dir, "logs"),  # è®¾ç½®æ—¥å¿—ç›®å½•
            remove_unused_columns=False,  # we supply our own collator
            # evaluation_strategy="steps",
            save_strategy="steps",
            logging_steps=args.logging_steps,
            # eval_steps=args.eval_steps,
            save_steps=args.save_steps,
            load_best_model_at_end=False,  # ç¦ç”¨è‡ªåŠ¨åŠ è½½æœ€ä½³æ¨¡å‹ï¼Œé¿å…eval_lossé”™è¯¯
            per_device_train_batch_size=args.per_device_train_batch_size,
            # per_device_eval_batch_size=2,
            learning_rate=args.learning_rate,
            num_train_epochs=args.num_train_epochs,
            fp16=args.fp16,
            bf16=args.bf16,
            gradient_accumulation_steps=args.gradient_accumulation_steps,
            deepspeed=args.deepspeed,
            dataloader_num_workers=args.dataloader_num_workers,  # å¢åŠ æ•°æ®åŠ è½½å¹¶è¡Œåº¦
            save_total_limit=args.save_total_limit,  # åªä¿ç•™æœ€è¿‘3ä¸ªcheckpoint
            prediction_loss_only=False,  # ä¿®æ”¹ä¸ºFalseä»¥è¾“å‡ºeval loss
            label_names=["speech_token"], # æŒ‡å®šlabel name
            save_safetensors=False,  # å…³é—­safetensorsï¼Œé¿å…å…±äº«æƒé‡ä¿å­˜æŠ¥é”™
        )

        # åˆ›å»ºæ¨¡å‹æ„ŸçŸ¥çš„æ•°æ®æ‰¹å¤„ç†å™¨
        data_collator = ModelAwareDataCollator(args.model)
        
        if args.model == "llm":
            logging.info("ğŸ¯ LLMæ¨¡å‹è®­ç»ƒï¼šä»…æå–speech_tokenï¼Œè·³è¿‡speech_featå’Œembeddingä»¥ä¼˜åŒ–æ€§èƒ½")
        elif args.model == "flow":
            logging.info("ğŸ¯ FLOWæ¨¡å‹è®­ç»ƒï¼šæå–å®Œæ•´ç‰¹å¾ï¼ˆspeech_feat, embedding, speech_tokenï¼‰")
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            # eval_dataset=eval_dataset,
            data_collator=data_collator,
            tokenizer=tokenizer if tokenizer is not None and args.model != "llm" else None,
        )

        # å¼€å§‹è®­ç»ƒ
        logging.info("Training...")
        trainer.train()
        logging.info("Training completed. Saving model...")
        trainer.save_model()
        logging.info("Saving completed. All Finished")
        
    except Exception as e:
        logging.error(f"âŒError training: {e}")
        raise e

if __name__ == "__main__":
    main()
