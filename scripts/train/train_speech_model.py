# -*- coding: utf-8 -*-
"""train_with_trainer.py

Rewrite of the original custom training loop to leverage Hugging Face ğŸ¤— `Trainer` & `TrainingArguments`.
The script keeps most domain-specific preprocessing logic (tokenization, audio â†’ mel, etc.) but
outsources distributed training, mixed precision, gradient accumulation, DeepSpeed integration,
checkpointing and tensorboard logging to the Transformers ecosystem.

Key features
------------
* Works with either the LLM or FLOW model defined in your YAML (`configs[args.model]`).
* Uses `datasets` map-style preprocessing so that all ranks benefit from cached results.
* `Trainer` handles â” fp16/bf16, ZeRO/DeepSpeed, DDP, gradient accumulation, evaluation, saving â€¦
* Metric computation stub provided â€“ adapt for your task.

Typical usage
-------------
python -m torch.distributed.run --nproc_per_node 8 train_with_trainer.py \
  --config path/to/config.yaml \
  --model llm \
  --train_data /path/train_ds \
  --cv_data /path/val_ds \
  --output_dir ckpt/llm_trainer \
  --deepspeed path/to/ds_config.json  # optional

See the ArgumentParser at the bottom for the full set of CLI switches.
"""
from __future__ import annotations

import argparse
import logging
import os
import sys
from pathlib import Path
from typing import Dict, List, Union, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent.absolute()
third_party_dir = project_root / "third_party"

sys.path.insert(0, str(project_root))
sys.path.insert(0, str(third_party_dir))

import numpy as np
import torch
import torchaudio
from datasets import load_from_disk, concatenate_datasets, Dataset, Audio
from hyperpyyaml import load_hyperpyyaml
from torch.nn.utils.rnn import pad_sequence
from transformers import (
    Trainer,
    TrainingArguments,
    HfArgumentParser,
    PreTrainedTokenizerBase,
    default_data_collator,
)

# ---------- Domain-specific imports ---------- #
from scripts.third_party.cosyvoice.tokenizer.tokenizer import get_qwen_tokenizer  
from scripts.third_party.matcha.utils.audio import mel_spectrogram

# -----------------------------------------------------------------------------
# Data preparation helpers
# -----------------------------------------------------------------------------

USEFUL_COLUMNS_LLM = ["text", "speech_token"]
USEFUL_COLUMNS_FLOW = ["speech_token", "audio", "embedding"]


def prepare_dataset_llm(ds: Dataset, tokenizer: PreTrainedTokenizerBase) -> Dataset:
    """Tokenise text + keep speech tokens; filter by length."""

    def _map(example):
        tokens = tokenizer.encode(example["text"], allowed_special="all")
        text_token = torch.tensor(tokens, dtype=torch.int32)
        text_token_len = len(text_token)
        speech_token = torch.tensor(example["speech_token"], dtype=torch.int32)
        speech_token_len = len(speech_token)
        return {
            "text_token": text_token,
            "text_token_len": text_token_len,
            "speech_token": speech_token,
            "speech_token_len": speech_token_len,
        }

    ds = ds.remove_columns([c for c in ds.column_names if c not in USEFUL_COLUMNS_LLM])
    ds = ds.map(_map, remove_columns=["text", "speech_token"], num_proc=os.cpu_count())
    ds = ds.filter(lambda ex: ex["speech_token_len"] > 25)
    return ds


def _audio_to_features(audio_dict, target_sr: int = 24000):
    wav = torch.from_numpy(audio_dict["array"]).unsqueeze(0).float()
    if audio_dict["sampling_rate"] != target_sr:
        wav = torchaudio.transforms.Resample(audio_dict["sampling_rate"], target_sr)(wav)
    estimated_frames = (wav.shape[-1] - 480) // 480 + 1
    if estimated_frames % 2 == 1:
        # å¦‚æœé¢„ä¼°é•¿åº¦æ˜¯å¥‡æ•°ï¼Œè°ƒæ•´éŸ³é¢‘é•¿åº¦
        padding_needed = 480  # æ·»åŠ ä¸€ä¸ªhop_sizeçš„é•¿åº¦
        wav = torch.nn.functional.pad(wav, (0, padding_needed))
    mel = mel_spectrogram(wav, 1920, 80, target_sr, 480, 1920, 0, 8000, False)
    mel = mel.squeeze(0).transpose(0, 1)
    return mel


def prepare_dataset_flow(ds: Dataset) -> Dataset:
    """Keep raw data as-is, defer all processing to collator for maximum speed."""
    
    # åªä¿ç•™å¿…è¦çš„åˆ—ï¼Œä¸åšä»»ä½•æ•°æ®è½¬æ¢
    ds = ds.remove_columns([c for c in ds.column_names if c not in USEFUL_COLUMNS_FLOW])
    
    return ds


# -----------------------------------------------------------------------------
# Data collator (pads variable-length tensors manually to keep Trainer happy)
# -----------------------------------------------------------------------------

def collate_fn(features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
    batch = {}
    
    # å¤„ç†éŸ³é¢‘æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if "audio" in features[0]:
        # è¾¹è®­è¾¹å¤„ç†ï¼šåœ¨è¿™é‡Œè¿›è¡ŒéŸ³é¢‘åˆ°melè°±å›¾çš„è½¬æ¢
        speech_feats = []
        speech_feat_lens = []
        
        for f in features:
            mel = _audio_to_features(f["audio"]).to(torch.bfloat16)
            speech_feats.append(mel)
            speech_feat_lens.append(len(mel))
        
        # å¯¹melè°±å›¾è¿›è¡Œpadding
        batch["speech_feat"] = pad_sequence(speech_feats, batch_first=True)
        batch["speech_feat_len"] = torch.tensor(speech_feat_lens)
    
    # å¤„ç† text_token
    if "text_token" in features[0]:
        text_tokens = []
        text_token_lens = []
        for f in features:
            text_token = f["text_token"]
            if not isinstance(text_token, torch.Tensor):
                text_token = torch.tensor(text_token, dtype=torch.int32)
            else:
                text_token = text_token.int()
            text_tokens.append(text_token)
            text_token_lens.append(len(text_token))
        batch["text_token"] = pad_sequence(text_tokens, batch_first=True)
        batch["text_token_len"] = torch.tensor(text_token_lens)


    # å¤„ç†speech_tokenæ•°æ®
    if "speech_token" in features[0]:
        speech_tokens = []
        speech_token_lens = []
        
        for f in features:
            # åœ¨è¿™é‡Œè¿›è¡Œæ•°æ®ç±»å‹è½¬æ¢
            speech_token = torch.tensor(f["speech_token"], dtype=torch.int32)
            speech_tokens.append(speech_token)
            speech_token_lens.append(len(speech_token))
        
        batch["speech_token"] = pad_sequence(speech_tokens, batch_first=True)
        batch["speech_token_len"] = torch.tensor(speech_token_lens)
    
    # å¤„ç†speaker embedding
    if "embedding" in features[0]:
        embeddings = []
        for f in features:
            emb = torch.tensor(f["embedding"], dtype=torch.bfloat16)
            embeddings.append(emb)
        batch["embedding"] = torch.stack(embeddings)
    
    # å¤„ç†å…¶ä»–æ•°æ®
    for key in features[0].keys():
        if key in ["audio", "speech_token", "embedding"]:  # è·³è¿‡å·²å¤„ç†çš„æ•°æ®
            continue
        if isinstance(features[0][key], torch.Tensor):
            batch[key] = pad_sequence([f[key] for f in features], batch_first=True)
        else:
            batch[key] = torch.tensor([f[key] for f in features])
    return batch


# -----------------------------------------------------------------------------
# Metric stub â€“ adapt to your task
# -----------------------------------------------------------------------------
from sklearn.metrics import accuracy_score  # ä¾‹å¦‚ä½ æƒ³åŠ ä¸ª accuracyï¼Œå¯ä»¥æŒ‰éœ€æ›¿æ¢
from transformers import EvalPrediction


# -----------------------------------------------------------------------------
# Main entry
# -----------------------------------------------------------------------------


def main():
    parser = argparse.ArgumentParser()
    # ---- high-level script args ---- #
    parser.add_argument("--config", type=str, required=True, help="YAML containing model + train cfg")
    parser.add_argument("--model", choices=["llm", "flow"], required=True)
    parser.add_argument("--model_ckpt", type=str, required=True, help="model checkpoint path")
    parser.add_argument("--tokenizer_path", type=str, required=True, help="tokenizer path")
    parser.add_argument("--train_data", type=str, required=True, help="comma-separated dataset paths")
    parser.add_argument("--cv_data", type=str, required=False, help="comma-separated validation dataset paths")
    parser.add_argument("--output_dir", type=str, required=True)

    # ---- pass-through TrainingArguments ---- #
    parser.add_argument("--per_device_train_batch_size", type=int, default=4)
    parser.add_argument("--per_device_eval_batch_size", type=int, default=4)
    parser.add_argument("--learning_rate", type=float, default=1e-4)
    parser.add_argument("--num_train_epochs", type=int, default=10)
    parser.add_argument("--fp16", action="store_true", default=False)
    parser.add_argument("--bf16", action="store_true", default=True)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1)
    parser.add_argument("--deepspeed", type=str, default=None, help="DeepSpeed json config path")

    parser.add_argument("--enable_lora", action="store_true", default=False)
    parser.add_argument("--lora_r", type=int, default=64)
    parser.add_argument("--lora_alpha", type=int, default=128)
    parser.add_argument("--lora_dropout", type=float, default=0.05)
    parser.add_argument("--lora_bias", type=str, default="none")
    parser.add_argument("--lora_target_modules", type=list, default=["q_proj", "v_proj", "k_proj"])

    parser.add_argument("--logging_steps", type=int, default=50)
    parser.add_argument("--eval_steps", type=int, default=1000)
    parser.add_argument("--save_steps", type=int, default=2000)
    parser.add_argument("--save_total_limit", type=int, default=None)
    parser.add_argument("--dataloader_num_workers", type=int, default=8)
    parser.add_argument("--auto_val_split", action="store_true", default=False, help="è‡ªåŠ¨åˆ’åˆ†éªŒè¯é›†")
    parser.add_argument("--val_split_ratio", type=float, default=0.05, help="éªŒè¯é›†æ¯”ä¾‹")

    args, unknown = parser.parse_known_args()

    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
    logging.info("Loading config â€¦")

    # ------------------------------------------------------------------
    # Build model from YAML (keeps original cosyvoice behaviour)
    # ------------------------------------------------------------------
    with open(args.config, "r") as f:
        cfg = load_hyperpyyaml(f, overrides={})
    model = cfg[args.model]

    # ------------------------------------------------------------------
    # Data
    # ------------------------------------------------------------------
    try:
        logging.info("Preprocessing datasets â€¦")
        
        # å‚æ•°éªŒè¯ï¼šéè‡ªåŠ¨åˆ’åˆ†æ—¶éœ€è¦cv_data
        if not args.auto_val_split and (not args.cv_data or not args.cv_data.strip()):
            raise ValueError("å½“æœªå¯ç”¨è‡ªåŠ¨åˆ’åˆ†éªŒè¯é›†æ—¶ï¼Œ--cv_data å‚æ•°æ˜¯å¿…éœ€çš„")
            
        train_paths = args.train_data.split(",")
        tokenizer = get_qwen_tokenizer(
            token_path=args.tokenizer_path, skip_special_tokens=True
        ) if args.model == "llm" else None
        model_state = torch.load(args.model_ckpt)
        if args.model == "llm":
            if 'epoch' in model_state:
                model_state.pop('epoch')
            if 'step' in model_state:
                model_state.pop('step')
        model.load_state_dict(model_state)

        if args.auto_val_split:
            # åªåŠ è½½è®­ç»ƒé›†ï¼Œè‡ªåŠ¨åˆ’åˆ†
            train_dss = [load_from_disk(p) for p in train_paths]
            if args.model == "llm":
                train_dss = [prepare_dataset_llm(ds, tokenizer) for ds in train_dss]
            else:
                train_dss = [prepare_dataset_flow(ds) for ds in train_dss]
            full_dataset = concatenate_datasets(train_dss).shuffle(seed=42)
            val_size = int(len(full_dataset) * args.val_split_ratio)
            train_size = len(full_dataset) - val_size
            train_dataset = full_dataset.select(range(train_size))
            eval_dataset = full_dataset.select(range(train_size, train_size + val_size))
            logging.info(f"è‡ªåŠ¨åˆ’åˆ†éªŒè¯é›†: è®­ç»ƒé›† {train_size}ï¼ŒéªŒè¯é›† {val_size}")
        else:
            val_paths = args.cv_data.split(",")
            train_dss = [load_from_disk(p) for p in train_paths]
            val_dss = [load_from_disk(p) for p in val_paths]
            # ç»Ÿä¸€éŸ³é¢‘æ ¼å¼ï¼Œé¿å…concatenate_datasetsæ—¶å‡ºé”™
            if args.model == "flow":
                logging.info("Unifying audio format across datasets for FLOW...")
                train_dss = [ds.cast_column("audio", Audio(sampling_rate=24000, mono=True, decode=True)) for ds in train_dss]
                val_dss = [ds.cast_column("audio", Audio(sampling_rate=24000, mono=True, decode=True)) for ds in val_dss]
            if args.model == "llm":
                logging.info("Preparing LLM dataset...")
                train_dss = [prepare_dataset_llm(ds, tokenizer) for ds in train_dss]
                val_dss = [prepare_dataset_llm(ds, tokenizer) for ds in val_dss]
            else:
                train_dss = [prepare_dataset_flow(ds) for ds in train_dss]
                val_dss = [prepare_dataset_flow(ds) for ds in val_dss]
            train_dataset = concatenate_datasets(train_dss).shuffle(seed=42)
            eval_dataset = concatenate_datasets(val_dss).shuffle(seed=42)
    except Exception as e:
        logging.error(f"âŒError preprocessing datasets: {e}")
        raise e

    # ------------------------------------------------------------------
    # TrainingArguments & Trainer
    # ------------------------------------------------------------------
    try:
        if args.enable_lora:
            from peft import LoraConfig, get_peft_model
            lora_config = LoraConfig(
                r=args.lora_r,
                lora_alpha=args.lora_alpha,
                target_modules=args.lora_target_modules,
                lora_dropout=args.lora_dropout,
                bias=args.lora_bias,
            )
            model = get_peft_model(model, lora_config)

        logging.info("Initialising Trainer â€¦")
        training_args = TrainingArguments(
            output_dir=args.output_dir,
            remove_unused_columns=False,  # we supply our own collator
            evaluation_strategy="steps",
            save_strategy="steps",
            logging_steps=args.logging_steps,
            eval_steps=args.eval_steps,
            save_steps=args.save_steps,
            load_best_model_at_end=False,  # ç¦ç”¨è‡ªåŠ¨åŠ è½½æœ€ä½³æ¨¡å‹ï¼Œé¿å…eval_lossé”™è¯¯
            per_device_train_batch_size=args.per_device_train_batch_size,
            per_device_eval_batch_size=args.per_device_eval_batch_size,
            learning_rate=args.learning_rate,
            num_train_epochs=args.num_train_epochs,
            fp16=args.fp16,
            bf16=args.bf16,
            gradient_accumulation_steps=args.gradient_accumulation_steps,
            deepspeed=args.deepspeed,
            dataloader_num_workers=args.dataloader_num_workers,  # å¢åŠ æ•°æ®åŠ è½½å¹¶è¡Œåº¦
            save_total_limit=args.save_total_limit,  # åªä¿ç•™æœ€è¿‘3ä¸ªcheckpoint
            prediction_loss_only=False,  # ä¿®æ”¹ä¸ºFalseä»¥è¾“å‡ºeval loss
            label_names=["speech_token"], # æŒ‡å®šlabel name
        )

        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            data_collator=collate_fn,
            tokenizer=tokenizer if tokenizer is not None and args.model != "llm" else None,
        )

        # ------------------------------------------------------------------
        # Training
        # ------------------------------------------------------------------
        logging.info("Training...")
        trainer.train()
        logging.info("Training completed. Saving model...")
        trainer.save_model()
        logging.info("Saving completed. All Finished")
        
    except Exception as e:
        logging.error(f"âŒError training: {e}")
        raise e

if __name__ == "__main__":
    main()
