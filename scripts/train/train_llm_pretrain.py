#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸“ç”¨ï¼šLLM é¢„è®­ç»ƒè„šæœ¬ï¼ˆCosyVoice3LM MTP pretrainï¼‰

å‚è€ƒï¼šscripts/train/train_speech_model.py

ç‰¹ç‚¹ï¼š
- åªè®­ç»ƒ LLMï¼ˆä¸åŒ…å« flow/ganï¼‰
- ä» hyperpyyaml æ„å»ºæ¨¡å‹ï¼ˆæ¨èä½¿ç”¨ pretrained_models/Fun-CosyVoice3-0.5B/cosyvoice3_mtp_pretrain.yamlï¼‰
- å…è®¸ä»æ—§ ckpt åŠ è½½ï¼ˆstrict=Falseï¼‰ï¼Œç”¨äºé¦–æ¬¡å¼•å…¥ mtp_block çš„æƒé‡è¡¥é½åç»§ç»­è®­ç»ƒ
- instruct_token / instruct_token_len æš‚æ—¶ä¸éœ€è¦ï¼šåœ¨ data_collator ä¸­è‡ªåŠ¨è¡¥ç©ºå ä½ï¼Œé¿å… forward å– key æŠ¥é”™
- dataset ä»…æœ‰ text/audioï¼šspeech_token åœ¨ collator ä¸­ä½¿ç”¨ ONNX ä»éŸ³é¢‘å®æ—¶æå–
"""

from __future__ import annotations

import argparse
import logging
import os
import sys
from pathlib import Path
from typing import Any, Dict, List

import numpy as np
import onnxruntime as ort
import torch
from torch import nn
import torchaudio
import whisper
from datasets import load_from_disk, concatenate_datasets, Dataset, Audio
from hyperpyyaml import load_hyperpyyaml
from torch.nn.utils.rnn import pad_sequence
from transformers import Trainer, TrainingArguments, PreTrainedTokenizerBase

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„ï¼ˆä¸ train_speech_model.py ä¿æŒä¸€è‡´ï¼‰
project_root = Path(__file__).parent.parent.parent.absolute()
third_party_dir = project_root / "server/model_utils"
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(third_party_dir))

from server.model_utils.cosyvoice.tokenizer.tokenizer import get_qwen_tokenizer


USEFUL_COLUMNS_LLM = ["text", "audio"]


def prepare_dataset_llm_pretrain(ds: Dataset) -> Dataset:
    """ä¿ç•™ LLM pretrain æ‰€éœ€åˆ—ï¼Œå‡å°‘ IO/å†…å­˜ã€‚"""
    ds = ds.remove_columns([c for c in ds.column_names if c not in USEFUL_COLUMNS_LLM])
    ds = ds.cast_column("audio", Audio(decode=True, sampling_rate=16000))
    return ds


_TOKENIZER_SESSION: ort.InferenceSession | None = None
_TOKENIZER_SESSION_KEY: str | None = None


def _get_onnx_tokenizer_session(
    onnx_path: str,
    use_cuda: bool,
    device_id: int,
    intra_op_num_threads: int = 1,
) -> ort.InferenceSession:
    """æ¯ä¸ªè¿›ç¨‹æ‡’åŠ è½½ä¸€ä»½ ONNX Sessionï¼ˆDataLoader worker å„è‡ªåˆå§‹åŒ–ï¼‰ã€‚"""
    global _TOKENIZER_SESSION, _TOKENIZER_SESSION_KEY
    # è‹¥ç¯å¢ƒä¸æ”¯æŒ CUDA providerï¼Œè¿™é‡Œä¼šè‡ªåŠ¨å›é€€åˆ° CPU provider
    available = set(ort.get_available_providers())
    effective_use_cuda = bool(use_cuda) and ("CUDAExecutionProvider" in available)
    if bool(use_cuda) and not effective_use_cuda:
        logging.warning(
            "onnxruntime æœªæ£€æµ‹åˆ° CUDAExecutionProviderï¼ˆavailable=%sï¼‰ï¼Œå°†è‡ªåŠ¨ä½¿ç”¨ CPUExecutionProviderã€‚",
            ",".join(sorted(available)),
        )

    key = f"{onnx_path}|cuda={effective_use_cuda}|dev={device_id}|intra={intra_op_num_threads}"
    if _TOKENIZER_SESSION is not None and _TOKENIZER_SESSION_KEY == key:
        return _TOKENIZER_SESSION

    so = ort.SessionOptions()
    so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
    so.intra_op_num_threads = int(intra_op_num_threads)

    if effective_use_cuda:
        # providers = [("CUDAExecutionProvider", {"device_id": int(device_id)})]
        providers = ["CPUExecutionProvider"]
    else:
        providers = ["CPUExecutionProvider"]

    _TOKENIZER_SESSION = ort.InferenceSession(onnx_path, sess_options=so, providers=providers)
    _TOKENIZER_SESSION_KEY = key
    return _TOKENIZER_SESSION


def _load_audio_16k_mono(audio_info: Any) -> torch.Tensor:
    """datasets.Audio decode åçš„ dictï¼ˆarray/sampling_rateï¼‰â†’ (1,T) float32 16k mono"""
    if isinstance(audio_info, dict) and "array" in audio_info:
        wav = torch.tensor(audio_info["array"], dtype=torch.float32)
        sr = int(audio_info.get("sampling_rate", 16000))
        if wav.dim() == 1:
            wav = wav.unsqueeze(0)
        elif wav.dim() == 2 and wav.size(0) > 1:
            wav = wav.mean(dim=0, keepdim=True)
        if sr != 16000:
            wav = torchaudio.transforms.Resample(sr, 16000)(wav)
        return wav

    if isinstance(audio_info, dict) and "path" in audio_info:
        path = audio_info["path"]
    else:
        path = str(audio_info)
    wav, sr = torchaudio.load(path)
    if wav.dim() == 2 and wav.size(0) > 1:
        wav = wav.mean(dim=0, keepdim=True)
    if sr != 16000:
        wav = torchaudio.transforms.Resample(sr, 16000)(wav)
    return wav.to(torch.float32)


def _extract_speech_token_from_audio(audio_info: Any, onnx_session: ort.InferenceSession) -> List[int]:
    wav_16k = _load_audio_16k_mono(audio_info)
    mel = whisper.log_mel_spectrogram(wav_16k, n_mels=128)
    ort_inputs = {
        onnx_session.get_inputs()[0].name: mel.cpu().numpy(),
        onnx_session.get_inputs()[1].name: np.array([mel.shape[2]], dtype=np.int32),
    }
    tokens = onnx_session.run(None, ort_inputs)[0].flatten().tolist()
    return tokens


class LlmPretrainDataCollator:
    """åªæ„é€  CosyVoice3LM.forward æ‰€éœ€å­—æ®µï¼ˆå¹¶è¡¥é½ instruct_* å ä½ï¼‰ã€‚"""

    def __init__(
        self,
        tokenizer: PreTrainedTokenizerBase | None,
        tokenizer_onnx_path: str,
        onnx_use_cuda: bool,
        onnx_device_id: int,
        ort_intra_op_num_threads: int = 1,
    ):
        self.tokenizer = tokenizer
        self.tokenizer_onnx_path = tokenizer_onnx_path
        self.onnx_use_cuda = onnx_use_cuda
        self.onnx_device_id = onnx_device_id
        self.ort_intra_op_num_threads = ort_intra_op_num_threads

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        batch: Dict[str, torch.Tensor] = {}

        # -------- text_token / text_token_len --------
        text_tokens: List[torch.Tensor] = []
        text_token_lens: List[int] = []
        if "text_token" in features[0]:
            for f in features:
                tt = f["text_token"]
                if not isinstance(tt, torch.Tensor):
                    tt = torch.tensor(tt, dtype=torch.long)
                tt = tt.long()
                text_tokens.append(tt)
                text_token_lens.append(int(tt.numel()))
        elif "text" in features[0]:
            if self.tokenizer is None:
                raise ValueError("æ•°æ®åªæœ‰ text å­—æ®µä½†æœªæä¾› tokenizerï¼Œæ— æ³•ç”Ÿæˆ text_tokenã€‚")
            for f in features:
                ids = self.tokenizer.encode(f["text"], allowed_special="all")
                tt = torch.tensor(ids, dtype=torch.long)
                text_tokens.append(tt)
                text_token_lens.append(int(tt.numel()))
        else:
            raise ValueError("LLM pretrain éœ€è¦ text_token æˆ– text å­—æ®µã€‚")

        batch["text_token"] = pad_sequence(text_tokens, batch_first=True, padding_value=0)
        batch["text_token_len"] = torch.tensor(text_token_lens, dtype=torch.int64)

        # -------- speech_token / speech_token_lenï¼ˆä»éŸ³é¢‘å®æ—¶æå–ï¼‰--------
        if "audio" not in features[0]:
            raise ValueError("LLM pretrain éœ€è¦ audio å­—æ®µä»¥å®æ—¶æå– speech_tokenã€‚")

        onnx_session = _get_onnx_tokenizer_session(
            self.tokenizer_onnx_path,
            use_cuda=self.onnx_use_cuda,
            device_id=self.onnx_device_id,
            intra_op_num_threads=self.ort_intra_op_num_threads,
        )
        speech_tokens: List[torch.Tensor] = []
        speech_token_lens: List[int] = []
        for f in features:
            tokens = _extract_speech_token_from_audio(f["audio"], onnx_session)
            st = torch.tensor(tokens, dtype=torch.long)
            speech_tokens.append(st)
            speech_token_lens.append(int(st.numel()))
        batch["speech_token"] = pad_sequence(speech_tokens, batch_first=True, padding_value=0)
        batch["speech_token_len"] = torch.tensor(speech_token_lens, dtype=torch.int64)

        # -------- instruct_token / instruct_token_lenï¼ˆæš‚æ—¶ä¸éœ€è¦ï¼Œè¡¥ç©ºå ä½ï¼‰--------
        bsz = len(features)
        batch["instruct_token"] = torch.zeros((bsz, 0), dtype=torch.long)
        batch["instruct_token_len"] = torch.zeros((bsz,), dtype=torch.int64)

        return batch


class _TrainerForwardWrapper(nn.Module):
    """
    é€‚é… HuggingFace Trainer çš„è°ƒç”¨æ–¹å¼ï¼šTrainer ä¼šè°ƒç”¨ model(**batch)ã€‚
    ä½† CosyVoice3LM.forward æœŸæœ› forward(batch: dict, device: torch.device)ã€‚
    """

    def __init__(self, core_model: nn.Module):
        super().__init__()
        self.core_model = core_model

    def forward(self, **batch):  # type: ignore[override]
        # Trainer åœ¨ _prepare_inputs åï¼Œbatch tensor å·²ç»åœ¨æ­£ç¡® device ä¸Š
        any_tensor = next((v for v in batch.values() if isinstance(v, torch.Tensor)), None)
        device = any_tensor.device if any_tensor is not None else next(self.core_model.parameters()).device
        return self.core_model(batch, device)


def _load_state_dict_maybe_container(path: str) -> Dict[str, torch.Tensor]:
    obj = torch.load(path, map_location="cpu")
    if isinstance(obj, dict) and "state_dict" in obj and isinstance(obj["state_dict"], dict):
        return obj["state_dict"]
    if isinstance(obj, dict):
        return obj
    raise ValueError("ä¸æ”¯æŒçš„ checkpoint æ ¼å¼ï¼šæœŸæœ›ä¸º state_dict æˆ– {'state_dict': ...}")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--config",
        type=str,
        default="pretrained_models/Fun-CosyVoice3-0.5B/cosyvoice3_mtp_pretrain.yaml",
        help="hyperpyyaml é…ç½®è·¯å¾„",
    )
    parser.add_argument("--train_data", type=str, required=True, help="è®­ç»ƒæ•°æ®è·¯å¾„ï¼ˆload_from_diskï¼‰ï¼Œé€—å·åˆ†éš”")
    parser.add_argument("--cv_data", type=str, default="", help="éªŒè¯æ•°æ®è·¯å¾„ï¼ˆå¯é€‰ï¼‰ï¼Œé€—å·åˆ†éš”ï¼›ä¸ºç©ºåˆ™ä¸è¯„ä¼°")
    parser.add_argument("--output_dir", type=str, required=True, help="è¾“å‡ºç›®å½•")
    parser.add_argument(
        "--model_ckpt",
        type=str,
        default="",
        help="åˆå§‹æ¨¡å‹ checkpointï¼ˆstate_dict æˆ– {'state_dict':...}ï¼‰ã€‚è‹¥æŒ‡å®š --resume_from_checkpointï¼Œå¯ä¸ä¼ ã€‚",
    )
    parser.add_argument(
        "--resume_from_checkpoint",
        type=str,
        default="",
        help="ä» HuggingFace Trainer çš„ checkpoint ç›®å½•æ–­ç‚¹ç»­è®­ï¼ˆå¦‚ output_dir/checkpoint-10000ï¼‰ã€‚ä¼ ç©ºåˆ™ä¸å¯ç”¨ã€‚",
    )
    parser.add_argument("--qwen_pretrain_path", type=str, default="", help="Qwen2Encoder çš„ pretrain_path / tokenizer è·¯å¾„")
    parser.add_argument(
        "--tokenizer_onnx_path",
        type=str,
        default="jzx-ai-lab/HydraVox/speech_tokenizer_v2.onnx",
        help="speech tokenizer ONNX è·¯å¾„ï¼ˆä»éŸ³é¢‘å®æ—¶æå– speech_tokenï¼‰",
    )
    parser.add_argument("--onnx_use_cuda", action="store_true", default=True, help="ONNX tokenizer æ˜¯å¦ä½¿ç”¨ CUDAExecutionProvider")
    parser.add_argument("--onnx_device_id", type=int, default=None, help="ONNX CUDA device_idï¼ˆé»˜è®¤å– LOCAL_RANK/RANKï¼Œå¦åˆ™ 0ï¼‰")
    parser.add_argument("--ort_intra_op_num_threads", type=int, default=1)

    # å…è®¸ç”¨ yaml çš„ train_conf ä½œä¸ºé»˜è®¤å€¼ï¼›CLI æŒ‡å®šåˆ™è¦†ç›–
    parser.add_argument("--learning_rate", type=float, default=None)
    parser.add_argument("--num_train_epochs", type=int, default=None)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=None)
    parser.add_argument("--per_device_train_batch_size", type=int, default=8)
    parser.add_argument("--per_device_eval_batch_size", type=int, default=1)
    parser.add_argument("--max_grad_norm", type=float, default=None)
    parser.add_argument("--logging_steps", type=int, default=None)
    parser.add_argument("--save_steps", type=int, default=None)
    parser.add_argument("--save_total_limit", type=int, default=None)
    parser.add_argument("--warmup_steps", type=int, default=None)
    parser.add_argument("--bf16", action="store_true", default=True)
    parser.add_argument("--fp16", action="store_true", default=False)
    parser.add_argument("--deepspeed", type=str, default=None)
    args, _ = parser.parse_known_args()

    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
    logging.info("ğŸš€ LLM pretrain è„šæœ¬å¯åŠ¨")

    # 1) load yaml & build model
    with open(args.config, "r") as f:
        if not args.qwen_pretrain_path:
            model_dir = os.getenv("TTS_MODEL_DIR", "jzx-ai-lab/HydraVox")
            args.qwen_pretrain_path = os.path.join(model_dir, "CosyVoice-BlankEN")
        cfg = load_hyperpyyaml(
            f,
            overrides={
                "qwen_pretrain_path": args.qwen_pretrain_path,
            },
        )

    model = cfg["llm"]

    # 2) load ckpt / resume
    resume_path = str(args.resume_from_checkpoint).strip()
    if resume_path:
        if not os.path.exists(resume_path):
            raise FileNotFoundError(f"--resume_from_checkpoint è·¯å¾„ä¸å­˜åœ¨ï¼š{resume_path}")
        if not os.path.isdir(resume_path):
            raise ValueError(f"--resume_from_checkpoint éœ€è¦ä¼  checkpoint ç›®å½•ï¼ˆå¦‚ checkpoint-10000ï¼‰ï¼Œä½†å¾—åˆ°ï¼š{resume_path}")
        logging.info("å°†ä» Trainer checkpoint æ–­ç‚¹ç»­è®­ï¼š%s", resume_path)
    else:
        if not str(args.model_ckpt).strip():
            raise ValueError("æœªæŒ‡å®š --resume_from_checkpoint æ—¶ï¼Œå¿…é¡»æä¾› --model_ckpt ä½œä¸ºåˆå§‹æƒé‡ã€‚")
        model_state = _load_state_dict_maybe_container(args.model_ckpt)
        # å…¼å®¹ train_speech_model.py çš„ ckpt é‡Œå¯èƒ½å¸¦ epoch/step
        model_state.pop("epoch", None)
        model_state.pop("step", None)
        missing, unexpected = model.load_state_dict(model_state, strict=False)
        if missing:
            logging.warning(f"load_state_dict missing keys: {len(missing)}ï¼ˆç¤ºä¾‹ï¼š{missing[:5]}ï¼‰")
        if unexpected:
            logging.warning(f"load_state_dict unexpected keys: {len(unexpected)}ï¼ˆç¤ºä¾‹ï¼š{unexpected[:5]}ï¼‰")

    # 3) dataset
    train_paths = [p for p in args.train_data.split(",") if p.strip()]
    train_dss = [prepare_dataset_llm_pretrain(load_from_disk(p)) for p in train_paths]
    train_dataset = concatenate_datasets(train_dss).shuffle(seed=42)

    eval_dataset = None
    if args.cv_data.strip():
        val_paths = [p for p in args.cv_data.split(",") if p.strip()]
        val_dss = [prepare_dataset_llm_pretrain(load_from_disk(p)) for p in val_paths]
        eval_dataset = concatenate_datasets(val_dss).shuffle(seed=42)

    # 4) tokenizerï¼ˆä»…å½“æ•°æ®é‡Œæ˜¯ text è€Œé text_token æ—¶æ‰éœ€è¦ï¼‰
    tokenizer = get_qwen_tokenizer(token_path=args.qwen_pretrain_path, skip_special_tokens=True, version="cosyvoice3")

    # 5) training argsï¼ˆä» yaml.train_conf å–é»˜è®¤ï¼‰
    train_conf = cfg.get("train_conf", {}) if isinstance(cfg, dict) else {}
    optim_conf = train_conf.get("optim_conf", {}) if isinstance(train_conf, dict) else {}
    scheduler_conf = train_conf.get("scheduler_conf", {}) if isinstance(train_conf, dict) else {}

    learning_rate = args.learning_rate if args.learning_rate is not None else float(optim_conf.get("lr", 1e-5))
    num_train_epochs = args.num_train_epochs if args.num_train_epochs is not None else int(train_conf.get("max_epoch", 1))
    gradient_accumulation_steps = (
        args.gradient_accumulation_steps if args.gradient_accumulation_steps is not None else int(train_conf.get("accum_grad", 1))
    )
    per_device_train_batch_size = args.per_device_train_batch_size if args.per_device_train_batch_size is not None else int(train_conf.get("batch_size", 8))
    per_device_eval_batch_size = args.per_device_eval_batch_size if args.per_device_eval_batch_size is not None else 1
    max_grad_norm = args.max_grad_norm if args.max_grad_norm is not None else float(train_conf.get("grad_clip", 1.0))
    logging_steps = args.logging_steps if args.logging_steps is not None else int(train_conf.get("log_interval", 50))
    warmup_steps = args.warmup_steps if args.warmup_steps is not None else int(scheduler_conf.get("warmup_steps", 0))
    save_steps = args.save_steps if args.save_steps is not None else int(train_conf.get("save_per_step", 2000))

    # scheduler: yaml é‡Œ constantlrï¼Œè¿™é‡Œæ˜ å°„åˆ° HF çš„ constant/constant_with_warmup
    scheduler_name = str(train_conf.get("scheduler", "linear")).lower()
    if scheduler_name in ("constantlr", "constant"):
        lr_scheduler_type = "constant_with_warmup" if warmup_steps and warmup_steps > 0 else "constant"
    else:
        lr_scheduler_type = "linear"

    if save_steps <= 0:
        save_strategy = "no"
        save_steps = 0
    else:
        save_strategy = "steps"

    training_args = TrainingArguments(
        output_dir=args.output_dir,
        learning_rate=learning_rate,
        num_train_epochs=num_train_epochs,
        per_device_train_batch_size=per_device_train_batch_size,
        per_device_eval_batch_size=per_device_eval_batch_size,
        gradient_accumulation_steps=gradient_accumulation_steps,
        max_grad_norm=max_grad_norm,
        logging_steps=logging_steps,
        save_strategy=save_strategy,
        save_steps=save_steps,
        save_total_limit=args.save_total_limit,
        evaluation_strategy="no" if eval_dataset is None else "steps",
        eval_steps=1000,
        warmup_steps=warmup_steps,
        lr_scheduler_type=lr_scheduler_type,
        fp16=args.fp16,
        bf16=args.bf16,
        deepspeed=args.deepspeed,
        dataloader_num_workers=8,
        remove_unused_columns=False,  # we supply our own collator
        save_safetensors=False,  # å…³é—­safetensorsï¼Œé¿å…å…±äº«æƒé‡ä¿å­˜æŠ¥é”™
    )

    # 6) collator
    if args.onnx_device_id is None:
        env_rank = os.getenv("LOCAL_RANK", os.getenv("RANK", "0"))
        args.onnx_device_id = int(env_rank) if str(env_rank).isdigit() else 0

    data_collator = LlmPretrainDataCollator(
        tokenizer=tokenizer,
        tokenizer_onnx_path=args.tokenizer_onnx_path,
        onnx_use_cuda=bool(args.onnx_use_cuda),
        onnx_device_id=int(args.onnx_device_id),
        ort_intra_op_num_threads=int(args.ort_intra_op_num_threads),
    )

    # 7) trainer
    # é€‚é… Trainer çš„ forward(**batch) è°ƒç”¨
    model_for_trainer = _TrainerForwardWrapper(model)

    trainer = Trainer(
        model=model_for_trainer,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
        tokenizer=None,  # æœ¬è®­ç»ƒä¸ä¾èµ– HF tokenizer è‡ªåŠ¨å¤„ç†ï¼Œé¿å…é¢å¤–å‰¯ä½œç”¨
    )

    logging.info("Training...")
    trainer.train(resume_from_checkpoint=resume_path or None)
    logging.info("Training completed. Saving model...")
    trainer.save_model()
    logging.info("All Finished.")


if __name__ == "__main__":
    main()


