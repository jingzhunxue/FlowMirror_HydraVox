#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°†è®­ç»ƒä¿å­˜çš„ `pytorch_model.bin` è½¬æ¢ä¸º `model.pt`ï¼ˆé»˜è®¤è½¬ä¸º bf16ï¼‰ã€‚

é»˜è®¤å¤„ç†è·¯å¾„:
  cv3_llm_multihead_pretrain/checkpoint-10000/pytorch_model.bin

ç”¨æ³•ç¤ºä¾‹:
  python scripts/post_process/convert_checkpoint_bin_to_pt.py
  python scripts/post_process/convert_checkpoint_bin_to_pt.py -i /path/to/pytorch_model.bin -o /out/model.pt
  python scripts/post_process/convert_checkpoint_bin_to_pt.py -i checkpoints/run/pytorch_model.bin --keep-dtype
"""

import argparse
import sys
from pathlib import Path
from typing import Dict, Any

import torch


def convert_state_dict_to_bf16(state_dict: Dict[str, Any]) -> Dict[str, Any]:
    """å°† state_dict ä¸­çš„æµ®ç‚¹æƒé‡è½¬æ¢ä¸º bf16 ä»¥èŠ‚çœç©ºé—´ã€‚"""
    converted = {}
    for key, value in state_dict.items():
        if isinstance(value, torch.Tensor) and value.is_floating_point():
            try:
                converted[key] = value.to(torch.bfloat16)
            except Exception:
                # æŸäº›å¼ é‡å¯èƒ½ä¸æ”¯æŒè½¬æ¢ï¼Œä¿æŒåŸæ ·
                converted[key] = value
        else:
            converted[key] = value
    return converted


def normalize_state_dict(state_dict: Dict[str, Any]) -> Dict[str, Any]:
    """
    å¤„ç†å¸¸è§åŒ…è£…:
    - å¦‚æœå¤–å±‚åªæœ‰ core_model é”®ï¼Œå±•å¼€å…¶å†…éƒ¨å­—å…¸
    - å¦‚æœé”®åå‰ç¼€ä¸º core_model.ï¼Œå»æ‰å‰ç¼€
    """
    if "core_model" in state_dict and isinstance(state_dict["core_model"], dict):
        state_dict = state_dict["core_model"]

    normalized = {}
    for key, value in state_dict.items():
        if isinstance(key, str) and key.startswith("core_model."):
            stripped = key[len("core_model.") :]
        else:
            stripped = key
        normalized[stripped] = value
    return normalized


def main():
    parser = argparse.ArgumentParser(
        description="å°† pytorch_model.bin è½¬æ¢ä¸º model.ptï¼ˆé»˜è®¤ bf16ï¼‰",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    parser.add_argument(
        "-i",
        "--input",
        type=str,
        default="cv3_llm_multihead_pretrain/checkpoint-10000/pytorch_model.bin",
        help="pytorch_model.bin æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "-o",
        "--output",
        type=str,
        help="è¾“å‡º model.pt è·¯å¾„ï¼ˆé»˜è®¤ä¸è¾“å…¥åŒç›®å½•ï¼‰",
    )
    parser.add_argument(
        "--keep-dtype",
        action="store_true",
        help="ä¿æŒåŸå§‹ dtypeï¼Œä¸è½¬æ¢ä¸º bf16",
    )

    args = parser.parse_args()

    input_path = Path(args.input)
    if not input_path.exists():
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        sys.exit(1)

    if input_path.name != "pytorch_model.bin":
        print(f"âš ï¸ è¾“å…¥æ–‡ä»¶åä¸æ˜¯ pytorch_model.bin: {input_path.name}")

    index_file = input_path.with_name("pytorch_model.bin.index.json")
    if index_file.exists():
        print("âŒ æ£€æµ‹åˆ°åˆ†ç‰‡æƒé‡ï¼ˆpytorch_model.bin.index.jsonï¼‰ï¼Œè¯·å…ˆåˆå¹¶åå†è½¬æ¢")
        sys.exit(1)

    output_path = Path(args.output) if args.output else input_path.parent / "model.pt"
    output_path.parent.mkdir(parents=True, exist_ok=True)

    try:
        print(f"ğŸ“¥ æ­£åœ¨åŠ è½½æƒé‡: {input_path}")
        state = torch.load(str(input_path), map_location="cpu")
        if not isinstance(state, dict):
            print("âŒ æƒé‡æ–‡ä»¶æ ¼å¼ä¸ç¬¦åˆé¢„æœŸï¼ˆéœ€è¦ state_dict å­—å…¸ï¼‰")
            sys.exit(1)

        state = normalize_state_dict(state)

        if args.keep_dtype:
            converted = state
            print("â„¹ï¸ ä¿æŒåŸå§‹ dtypeï¼Œä¸æ‰§è¡Œ bf16 è½¬æ¢")
        else:
            converted = convert_state_dict_to_bf16(state)
            print("âœ… å·²å°†å¯è½¬æ¢çš„æµ®ç‚¹å¼ é‡è½¬ä¸º bf16")

        torch.save(converted, str(output_path))
        print(f"ğŸ‰ è½¬æ¢å®Œæˆ: {output_path}")
    except Exception as exc:
        print(f"âŒ è½¬æ¢å¤±è´¥: {exc}")
        sys.exit(1)


if __name__ == "__main__":
    main()
