<div align="center">

<img src="assets/HydraVox.png" alt="HydraVox Logo" width="25%" />

# FlowMirror-HydraVox

**A natively accelerated TTS (text-to-speech) model with multi-head decoding, derived from CosyVoice.**

\[English] Â· [ç®€ä½“ä¸­æ–‡](README-zh.md) *(coming soon)*

</div>

---

<p align="center">
  <a href="#-highlights">Highlights</a> â€¢
  <a href="#-quickstart-1-minute">Quickstart</a> â€¢
  <a href="#webui">WebUI</a> â€¢
  <a href="#roadmap">Roadmap</a> â€¢
  <a href="#python-api">Python API</a> â€¢
  <a href="#models--weights">Models</a> â€¢
  <a href="#train--finetune">Train</a> â€¢
  <a href="#-architecture">Architecture</a> â€¢
  <a href="#-license">License</a>
</p>

<p align="center">
  <img alt="badge-python" src="https://img.shields.io/badge/Python-3.10%2B-blue" />
  <img alt="badge-pytorch" src="https://img.shields.io/badge/PyTorch-2.3%2B-red" />
  <img alt="badge-cuda" src="https://img.shields.io/badge/CUDA-12.x-76b900" />
  <img alt="badge-license" src="https://img.shields.io/badge/License-Apache--2.0-green" />
</p>

---

## âœ¨ Highlights

* **Multiâ€‘Head AR Decoding** â€” Predict multiple speech tokens per step â†’ **lower latency** and **higher throughput** under the same quality target.
* **Readyâ€‘toâ€‘use WebUI** â€” Inference, flash batch synthesis, fineâ€‘tuning(comming soon), speaker mgmt, logs/plots.
* **Hot-load LoRA for emotion/style**(comming soon) â€” Load/unload adapters at runtime per utterance; stack multiple with per-adapter scaling (e.g. `--lora happy.safetensors:0.6,energetic.safetensors:0.3`).
* **SFT implement** â€” SFT implement derived from CosyVoice2.0.
* **Reproducible scripts** â€” Oneâ€‘command demo and fully versionâ€‘locked configs.
* **CosyVoice2.0â€‘derived** â€” Clear deltas vs upstream; compatible data formats where possible.

> **Responsible use:** Please do not clone or impersonate voices without explicit consent. See [Safety & Use Policy](#-safety--responsible-use).

---

## ğŸ”Š Samples & Demo

* **Audio samples**: `assets/samples/æµªæµªå±±å°å¦–æ€ª-é‡çŒª.WAV`
* **Online demo**: `http://localhost:7890` â€” link to Space/website.

---

## ğŸš€ Quickstart (1 minute)

> **Prereqs**: Python 3.10+, FFmpeg installed and on PATH; NVIDIA GPU + CUDA 12.x recommended. CPU fallback supported (slower).

### From source

```bash
# 0) Clone
git clone https://github.com/jingzhunxue/FlowMirror-HydraVox.git
cd FlowMirror-HydraVox

# 1) Create conda env
conda create -n hydravox python=3.11

# 2) Install dependencies
pip install -r requirements.txt

# 3) Download model weights
modelscope download jzx-ai-lab/HydraVox --local_dir jzx-ai-lab/HydraVox

# 4) Create .env
cp .env.example .env
```

---
<a name="webui"></a>
## WebUI

Start:

```bash
python main.py --api-host 0.0.0.0 --api-port 7860 --with-ui
# API-only mode (no browser UI):
python main.py --api-host 0.0.0.0 --api-port 7860
```

Features:

* Text â†’ Speech, longâ€‘text chunking.
* **Data Process panel**: dataset browser, configs, live logs & curves.
* **Training/Finetune panel**: dataset browser, configs, live logs & curves.
* **Speaker manager**: add/rename/delete speakers, preview, embeddings.

Screenshots:

<p align="center">
  <img src="assets/ui-home.png" alt="HydraVox WebUI - Home" width="80%" />
  <img src="assets/ui-train.png" alt="HydraVox WebUI - Train" width="80%" />
  <!-- If the images are large, adjust width to 45%-49% to keep them on one line. -->
</p>

**Data directories** (defaults, overridable):

```
jzx-ai-lab/HydraVox      # model weights
logs/             # train/infer logs
```

---
<a name="roadmap"></a>
## Roadmap

...
* [ ] 2025/10
  - [ ] Stream inference support for HydraVox
  - [ ] Deepseek style Multi-Token-Pretiction Module implement for HydraVox which enable more powerful and stable inference
  
* [ ] 2025/09
  - [ ] flow-matching core update introducing a TTS-tailored paradigm

* [ ] 2025/08
  - [X] Release training ui tab and training scripts
  - [ ] Release LoRA hot-load and inference with pretrained emotion lora
---

## Python API

åŸºäº REST æ¥å£ï¼ˆFastAPIï¼Œé»˜è®¤å‰ç¼€ `/api/v1`ï¼‰ã€‚ä¸‹é¢ç»™å‡ºæœ€å°å¯ç”¨ Python è°ƒç”¨ç¤ºä¾‹ä¸å…³é”®å­—æ®µè¯´æ˜ã€‚

```python
import base64
import requests

BASE = "http://localhost:8888/api/v1"

def load_pt(llm_pt: str, flow_pt: str):
    resp = requests.post(f"{BASE}/load_pt", json={
        "llm_pt": llm_pt,
        "flow_pt": flow_pt,
    }, timeout=120)
    resp.raise_for_status()
    print(resp.json())
    return resp.json()

def list_speakers():
    resp = requests.get(f"{BASE}/speakers", timeout=30)
    resp.raise_for_status()
    return resp.json()

def tts(text: str, speaker_id: str,
        output_format: str = "wav",
        last_prompt: bool = True,
        extra_params: dict | None = None):
    payload = {
        "text": text,
        "speaker_id": speaker_id,
        "output_format": output_format,
        "last_prompt": last_prompt,
        "extra_params": extra_params or {
            "top_p": 0.9,
            "top_k": 10,
            "win_size": 32,
            "tau_r": 0.2,
            "inference_head_num": 2
        }
    }
    resp = requests.post(f"{BASE}/tts", json=payload, timeout=90)
    resp.raise_for_status()
    data = resp.json()
    if not data.get("success", True):
        raise RuntimeError(data.get("error") or data.get("message"))
    audio_b64 = data["data"]["audio_base64"]
    with open(f"out_tts.{output_format}", "wb") as f:
        f.write(base64.b64decode(audio_b64))
    return data

def zero_shot(tts_text: str, prompt_text: str, prompt_wav_path: str,
              output_format: str = "wav",
              extra_params: dict | None = None):
    with open(prompt_wav_path, "rb") as f:
        prompt_audio_base64 = base64.b64encode(f.read()).decode("utf-8")
    payload = {
        "tts_text": tts_text,
        "prompt_text": prompt_text,
        "prompt_audio_base64": prompt_audio_base64,
        "output_format": output_format,
        "extra_params": extra_params or {
            "top_p": 0.9,
            "top_k": 10,
            "win_size": 32,
            "tau_r": 0.2,
            "inference_head_num": 2
        }
    }
    resp = requests.post(f"{BASE}/zero-shot", json=payload, timeout=120)
    resp.raise_for_status()
    data = resp.json()
    if not data.get("success", True):
        raise RuntimeError(data.get("error") or data.get("message"))
    with open(f"out_zero_shot.{output_format}", "wb") as f:
        f.write(base64.b64decode(data["data"]["audio_base64"]))
    return data

# Example usage
# load_pt("checkpoints/llm.pt", "checkpoints/flow.pt")
# speakers = list_speakers(); print(speakers)
# tts("ä»Šå¤©å¤©æ°”å¾ˆå¥½ã€‚", speaker_id="spk1")
# zero_shot("è¯·æŠŠä¸‹é¢æ–‡æœ¬è¯»å‡ºæ¥ï¼šä½ å¥½ä¸–ç•Œã€‚", "ä½ å¥½ï¼Œæˆ‘çš„å£°éŸ³æ˜¯è¿™æ ·çš„ã€‚", "assets/samples/prompt.wav")
```

**Key arguments**

- **POST `/api/v1/tts`**
  - **text**: å¾…åˆæˆæ–‡æœ¬ï¼ˆå¿…å¡«ï¼‰
  - **speaker_id**: è¯´è¯äºº IDï¼ˆå¿…å¡«ï¼Œå¯é€šè¿‡ `/speakers` æŸ¥è¯¢ï¼‰
  - **output_format**: è¾“å‡ºæ ¼å¼ï¼Œé»˜è®¤ `wav`
  - **last_prompt**: æ˜¯å¦ä½¿ç”¨ä¸Šä¸€æ®µéŸ³é¢‘ä½œä¸º zeroâ€‘shot æç¤ºï¼Œé»˜è®¤ `true`
  - **extra_params**: æ¨ç†è¶…å‚ï¼ˆå¯é€‰ï¼‰
    - `top_p`=0.9, `top_k`=10, `win_size`=32, `tau_r`=0.2, `inference_head_num`=2
  - **å“åº”**: `{ success, message, data: { audio_base64, sample_rate, format, duration, speaker_id, segments_info } }`

- **POST `/api/v1/zero-shot`**
  - **tts_text**: å¾…åˆæˆæ–‡æœ¬ï¼ˆå¿…å¡«ï¼‰
  - **prompt_text**: ä¸æç¤ºéŸ³é¢‘è¯­ä¹‰ç›¸åŒ¹é…/æè¿°çš„æ–‡æœ¬ï¼ˆå¿…å¡«ï¼‰
  - **prompt_audio_base64**: æç¤ºéŸ³é¢‘çš„ base64ï¼ˆå¿…å¡«ï¼‰
    - ä¾‹å¦‚ï¼š`base64.b64encode(open('prompt.wav','rb').read()).decode('utf-8')`
  - **output_format**: è¾“å‡ºæ ¼å¼ï¼Œé»˜è®¤ `wav`
  - **extra_params**: åŒä¸Š
  - **å“åº”**: `{ success, message, data: { audio_base64, sample_rate, format, duration, segments_info } }`

- **POST `/api/v1/load_pt`**
  - **llm_pt**: LLM æƒé‡è·¯å¾„ï¼ˆå¿…å¡«ï¼‰
  - **flow_pt**: Flow æƒé‡è·¯å¾„ï¼ˆå¿…å¡«ï¼‰
  - **å“åº”**: `{ success, message, data | error }`

- **GET `/api/v1/speakers`**
  - **ç”¨é€”**: æŸ¥è¯¢å¯ç”¨è¯´è¯äººåˆ—è¡¨æˆ–ä¿¡æ¯
  - **å“åº”**: è¿”å›è¯´è¯äººé›†åˆï¼ˆå®ç°å¯èƒ½è¿”å›ç›´æ¥åˆ—è¡¨ï¼Œæˆ–åŒ…è£…ä¸º `{ success, ... }`ï¼‰

æç¤ºä¸çº¦æŸï¼š
- æœåŠ¡å™¨ç«¯å¯¹ `/tts` è®¾æœ‰çº¦ 60s è¶…æ—¶ï¼›é•¿æ–‡æœ¬è¯·è‡ªè¡Œåˆ‡åˆ†æˆ–æå‡æœåŠ¡å™¨èµ„æºã€‚
- `prompt_audio_base64` åº”ä¸ºåŸå§‹éŸ³é¢‘æ–‡ä»¶çš„å­—èŠ‚è¿›è¡Œ Base64 ç¼–ç åçš„å­—ç¬¦ä¸²ã€‚
- è¿”å›çš„ `audio_base64` å¯ç›´æ¥ `base64.b64decode(...)` ä¿å­˜ä¸ºéŸ³é¢‘æ–‡ä»¶ã€‚

---

## Models & Weights

| Name                  | Params | Langs   | type   | Multiâ€‘Head | Link  |
| --------------------- | -----: | ------- | --------- | ---------: | ----- |
| hydravox-base-pretrained         | \~300M | zh/en   |AR-Transformer  |          5 | https://www.modelscope.cn/models/jzx-ai-lab/HydraVox/file/view/master/llm.pt |

> Download total weights by ```
modelscope download jzx-ai-lab/HydraVox --local_dir jzx-ai-lab/HydraVox```

---
## Train & Finetune

### ä½¿ç”¨ WebUI è¿›è¡Œæ•°æ®é¢„å¤„ç†ï¼ˆData Processï¼‰

æ‰“å¼€ WebUI åè¿›å…¥â€œğŸ“Š æ•°æ®å¤„ç†â€æ ‡ç­¾é¡µï¼Œæ”¯æŒä¸€é”®æµæ°´çº¿æˆ–é€é˜¶æ®µè¿è¡Œã€‚

- ä¸€é”®å¤„ç†ï¼šåœ¨â€œğŸš€ ä¸€é”®å¤„ç† - è‡ªåŠ¨è¿è¡Œå…¨éƒ¨å››ä¸ªé˜¶æ®µâ€ä¸­å¡«å†™â€œğŸ“ è¾“å…¥ç›®å½•â€â€œğŸ¤ é‡‡æ ·ç‡â€â€œâš ï¸ è¦†ç›–æ–‡ä»¶â€ï¼Œç‚¹å‡»â€œğŸš€ å¼€å§‹ä¸€é”®å¤„ç†â€ã€‚ç•Œé¢å°†ä¾æ¬¡æ‰§è¡Œå››ä¸ªé˜¶æ®µå¹¶å®æ—¶æ˜¾ç¤ºæ€»ä½“è¿›åº¦ã€çŠ¶æ€ä¸æ—¥å¿—ã€‚
- é˜¶æ®µä¸è¾“å‡ºç›®å½•çº¦å®šï¼š
  - é˜¶æ®µ1ï¼ˆæ ¼å¼è½¬æ¢ä¸é‡é‡‡æ ·ï¼‰â†’ è¾“å‡ºè‡³ `<è¾“å…¥>_resample`
  - é˜¶æ®µ2ï¼ˆVAD è¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼‰â†’ è¾“å‡ºè‡³ `<é˜¶æ®µ1è¾“å‡º>_vad`
  - é˜¶æ®µ3ï¼ˆASR è½¬å½•ï¼‰â†’ è¾“å‡ºè‡³ `<é˜¶æ®µ2è¾“å‡º>_asr`
  - é˜¶æ®µ4ï¼ˆæå–è¯­éŸ³è®­ç»ƒ Tokenï¼‰â†’ è¾“å‡ºè‡³ `<é˜¶æ®µ3è¾“å‡º>_token`
  - é˜¶æ®µ5ï¼ˆå¯é€‰ï¼Œæ•°æ®é›†åˆå¹¶ï¼‰â†’ å°†å¤šä¸ª HF æ•°æ®é›†ç›®å½•åˆå¹¶ä¿å­˜åˆ°æŒ‡å®šç›®å½•

é€é˜¶æ®µè¿è¡Œè¦ç‚¹ï¼ˆå¯¹åº”æŠ˜å é¢æ¿ï¼‰ï¼š
- é˜¶æ®µ 1 - æ ¼å¼è½¬æ¢ä¸é‡é‡‡æ ·
  - é€‰æ‹©è¾“å…¥/è¾“å‡ºç›®å½•ä¸é‡‡æ ·ç‡ï¼ˆé»˜è®¤ 16kHzï¼‰ï¼Œæ”¯æŒè¦†ç›–å·²å­˜åœ¨æ–‡ä»¶
  - é€šè¿‡â€œğŸ‘€ é¢„è§ˆå˜æ›´â€æŸ¥çœ‹æºæ–‡ä»¶â†’ç›®æ ‡æ–‡ä»¶æ˜ å°„ï¼Œç‚¹å‡»â€œâ–¶ï¸ å¼€å§‹å¤„ç†â€æ‰§è¡Œ
- é˜¶æ®µ 2 - VAD è¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆSileroï¼‰
  - å…³é”®å‚æ•°ï¼šç½®ä¿¡åº¦é˜ˆå€¼ã€æœ€çŸ­è¯­éŸ³/é™éŸ³(ms)ã€å‰åå¡«å……(ms)ã€æœ€çŸ­/æœ€é•¿ç‰‡æ®µ(s)
  - ç‚¹å‡»â€œâ–¶ï¸ å¼€å§‹å¤„ç†â€åæ˜¾ç¤ºåˆ†æ®µè¿›åº¦ä¸æ—¥å¿—
- é˜¶æ®µ 3 - ASR è¯­éŸ³è¯†åˆ«è½¬å½•
  - è®¾å¤‡é€‰æ‹©ï¼šè‡ªåŠ¨/CPU/GPUï¼›å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆGPU æ•°ï¼‰ï¼›å¯åˆ·æ–°è®¾å¤‡æ£€æµ‹
  - è¾“å‡ºä¸º HuggingFace æ•°æ®é›†ç›®å½•
- é˜¶æ®µ 4 - æå–è¯­éŸ³è®­ç»ƒ Token
  - è®¾å¤‡é€‰æ‹©ä¸å¹¶è¡Œè¿›ç¨‹é…ç½®åŒä¸Š
  - è¾“å‡ºä¸º Token åŒ–åçš„æ•°æ®é›†ç›®å½•
- é˜¶æ®µ 5 - æ•°æ®é›†åˆå¹¶ï¼ˆå¯é€‰ï¼‰
  - è¾“å…¥å¤šä¸ªæ•°æ®é›†ç›®å½•ï¼ˆè‹±æ–‡é€—å·åˆ†éš”ï¼‰ï¼Œåˆå¹¶åä¿å­˜åˆ°ç›®æ ‡ç›®å½•

æç¤ºï¼šç•Œé¢ä¼šè‡ªåŠ¨æ¢æµ‹ CUDA å¹¶æç¤º GPU æ•°ï¼›ASR/VAD é»˜è®¤ä»¥ 16kHz å¤„ç†ï¼›æ¯é˜¶æ®µå¸¦æœ‰çŠ¶æ€ä¸æ—¥å¿—ä¾¿äºæ’é”™ã€‚

### è®­ç»ƒï¼ˆWebUIï¼šğŸš€ æ¨¡å‹è®­ç»ƒï¼‰

åœ¨â€œğŸš€ æ¨¡å‹è®­ç»ƒâ€æ ‡ç­¾é¡µå®Œæˆä»¥ä¸‹è®¾ç½®å¹¶ç‚¹å‡»â€œå¼€å§‹è®­ç»ƒâ€ï¼š

- 1) æ•°æ®é›†é…ç½®
  - è®­ç»ƒæ•°æ®è·¯å¾„ï¼šé€‰æ‹©ç”±é˜¶æ®µ 3/4 äº§å‡ºçš„ HF æ•°æ®é›†ç›®å½•ï¼ˆå¦‚ `<...>_asr` æˆ– `<...>_token`ï¼‰
- 2) æ¨¡å‹é…ç½®
  - æ¨¡å‹ç±»å‹ï¼š`llm` æˆ– `flow`
  - æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„ï¼šå¦‚ `jzx-ai-lab/HydraVox/llm.pt`
  - åˆ†è¯å™¨è·¯å¾„ï¼šå¦‚ `jzx-ai-lab/HydraVox/CosyVoice-BlankEN`
  - è¾“å‡ºç›®å½•ï¼šå¦‚ `checkpoints/training_llm`
- 3) è®­ç»ƒå‚æ•°
  - æ‰¹æ¬¡å¤§å°ã€å­¦ä¹ ç‡ã€è®­ç»ƒè½®æ•°ã€ä¿å­˜é—´éš”(æ­¥æ•°)ã€æ—¥å¿—è®°å½•é—´éš”(logging_steps)ã€è¯„ä¼°é—´éš”(eval_steps)
  - éªŒè¯é›†ï¼šæ”¯æŒæŒ‰æ¯”ä¾‹è‡ªåŠ¨åˆ’åˆ†æˆ–æä¾›ç°æˆéªŒè¯é›†ï¼ˆæœªæ£€æµ‹åˆ°æ—¶è‡ªåŠ¨åˆ‡æ¢ä¸ºè‡ªåŠ¨åˆ’åˆ†ï¼‰
- 4) é«˜çº§é€‰é¡¹
  - å¯ç”¨ LoRA å¾®è°ƒï¼ˆå¯é€‰ï¼‰
  - ç²¾åº¦è®¾ç½®ï¼šBF16/FP16ï¼ˆä¸åŒæ¨¡å‹ç±»å‹ç»™å‡ºæ¨èï¼‰
- 5) è®¡ç®—èµ„æº
  - è®¡ç®—è®¾å¤‡ï¼šè‡ªåŠ¨/CPU/GPUï¼›å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆGPU æ•°ï¼‰ï¼›å¯æŒ‡å®š GPU IDs å¹¶åˆ·æ–°è®¾å¤‡æ£€æµ‹
- 6) è®­ç»ƒæ§åˆ¶ä¸å¯è§†åŒ–
  - æ—¥å¿—ï¼šé¢æ¿å®æ—¶è¾“å‡ºï¼Œå¹¶ä¿å­˜åˆ° `logs/training/train_<timestamp>.log`
  - æ›²çº¿ï¼šè‡ªåŠ¨ç”Ÿæˆå¹¶ä¿å­˜åˆ° `<è¾“å‡ºç›®å½•>/figure/training_plot.png`
  - æ”¯æŒâ€œç«‹å³åˆ·æ–°/å¼ºåˆ¶åˆ·æ–°â€å›¾è¡¨ä¸å®šæ—¶è‡ªåŠ¨åˆ·æ–°

è¯´æ˜ï¼šå†…éƒ¨ä½¿ç”¨ Accelerate å¯åŠ¨ï¼ŒæŒ‰ç²¾åº¦è®¾ç½®è‡ªåŠ¨é€‰æ‹© `fp16/bf16`ï¼›`logging_steps` ä¸ `eval_steps` å°†ä½“ç°åœ¨æ—¥å¿—ä¸æ›²çº¿çš„æ­¥æ•°åˆ»åº¦ä¸Šã€‚

### æ¨¡å‹ç®¡ç†

â€œæ¨¡å‹ç®¡ç†â€é¢æ¿æä¾›å¯¹è®­ç»ƒè¾“å‡ºçš„å¿«æ·æ“ä½œï¼š
- åˆ—è¡¨ï¼šæŒ‰æ—¶é—´å€’åºæ˜¾ç¤ºå·²å‘ç°çš„è¾“å‡ºè·¯å¾„ï¼ˆæ‰«æ `checkpoints/training_llm`ã€`checkpoints/training_flow`ã€`checkpoints/training`ã€`checkpoints`ã€`models`ã€`outputs`ã€`ckpt` ç­‰ç›®å½•ï¼Œè‡ªåŠ¨å¿½ç•¥ `runs/logs/figure`ï¼‰
- åˆ·æ–°åˆ—è¡¨ï¼šé‡æ–°æ‰«æç›®å½•
- åŠ è½½è·¯å¾„ï¼šå°†é€‰ä¸­çš„è·¯å¾„å›æ˜¾åˆ°è¾“å…¥æ¡†ï¼ˆä¾¿äºåç»­æ“ä½œï¼‰
- åˆ é™¤è·¯å¾„ï¼šå±é™©æ“ä½œï¼Œç›´æ¥åˆ é™¤æ‰€é€‰æ–‡ä»¶å¤¹ï¼ˆé™åˆ¶åˆ é™¤ç³»ç»Ÿæ–‡ä»¶å¤¹å¦‚ `runs/logs/figure`ï¼‰
- è½¬æ¢ä¸º `model.pt (bf16)`ï¼šå°†ç›®å½•ä¸‹çš„ `pytorch_model.bin` è½¬æ¢ä¸º `model.pt`
  - ä¸æ”¯æŒåˆ†ç‰‡ç´¢å¼• `.bin.index.json`ï¼Œéœ€å…ˆåˆå¹¶å†è½¬æ¢

æ³¨æ„ï¼šå¤§è§„æ¨¡å¤„ç†ä¸è®­ç»ƒå‰è¯·ç¡®ä¿ç£ç›˜ç©ºé—´ä¸ GPU èµ„æºå……è¶³ï¼›è‹¥è®­ç»ƒç»“æŸæœªè‡ªåŠ¨åˆ·æ–°æ›²çº¿ï¼Œå¯ç‚¹å‡»â€œâš¡ å¼ºåˆ¶åˆ·æ–°â€ã€‚

---

## License

* Code: **Apacheâ€‘2.0** *(example â€” update to your actual license)*.
* **Derived from CosyVoice** â€” see `NOTICE` and `LICENSE-THIRD-PARTY` for upstream licenses and modifications.

---

## ğŸ“š Citation

```bibtex
@software{hydravox2025,
  title = {FlowMirror-HydraVox: Multi-head AR TTS with Native Acceleration},
  author = {Your Name and Contributors},
  year = {2025},
  url = {https://github.com/your-org/FlowMirror-HydraVox}
}
```

---

## ğŸ™ Acknowledgements

* [**CosyVoice**](https://github.com/FunAudioLLM/CosyVoice) authors and contributors.
* [**Better & Faster Large Language Models via Multi-token Prediction**](https://arxiv.org/abs/2404.19737)
